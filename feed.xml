<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Allen Akinkunle | Data Science, Machine Learning</title>
    <description>The personal website of Allen Akinkunle</description>
    <link>http://allenkunle.me//</link>
    <atom:link href="http://allenkunle.me//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 11 Nov 2020 23:13:54 +0000</pubDate>
    <lastBuildDate>Wed, 11 Nov 2020 23:13:54 +0000</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>The Bias-Variance Decomposition Demystified</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The generalisation error of a machine learning algorithm measures how accurately the learning algorithm is able to predict the outcome of new data, unseen during training. The bias-variance decomposition shows the generalisation error as the sum of three terms: bias, variance, and the irreducible error.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn1&quot;&gt;
$$
  \tag{1} \text{Generalisation error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$
&lt;/p&gt;

&lt;p&gt;In many statistics and machine learning texts, this decomposition is just presented and its derivation skipped. In texts where it derived, it is often presented in a way that is inaccessible to many people. And why is it important to understand this derivation? Because the decomposition is a useful theoretical tool for understanding the performance of a learning algorithm. Understanding how bias and variance contribute to generalisation error helps us understand underfitting and overfitting.&lt;/p&gt;

&lt;p&gt;The goal of this article is to present the bias-variance decomposition in an accessible and easy-to-follow format. We will restrict our discussion of this decomposition to regression problems where mean squared error is used as the performance metric.&lt;/p&gt;

&lt;h3 id=&quot;a-quick-statistics-refresher&quot;&gt;A Quick Statistics Refresher&lt;/h3&gt;
&lt;p&gt;We will go over some statistics concepts that will aid our understanding of the other concepts we will discuss.&lt;/p&gt;

&lt;h4 id=&quot;expectation-of-a-random-variable&quot;&gt;Expectation of a random variable&lt;/h4&gt;
&lt;p&gt;The expectation&lt;sup id=&quot;fnref:expectation&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:expectation&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or expected value of a random variable $X$, written as $\mathbb{E}[X]$, is the &lt;strong&gt;mean&lt;/strong&gt; of a large number of observations of the random variable. The expectation has some basic properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The expectation of any constant $c$ is the constant:
    &lt;p class=&quot;math&quot; id=&quot;eqn2&quot;&gt;
 $$
 \tag{2} \begin{aligned}
   \mathbb{E}[c] = c
  \end{aligned}
 $$
 &lt;/p&gt;

    &lt;p&gt;$\mathbb{E}\big[\mathbb{E}[X]\big] = \mathbb{E}[X]$ because the expection of a random variable is a constant.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Linearity of expectations:&lt;/strong&gt; For any random variables $X$ and $Y$, the expectation of their sum is equal to the sum of their expectations:
    &lt;p class=&quot;math&quot; id=&quot;eqn3&quot;&gt;
  $$
  \tag{3} \begin{aligned}
    \mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]
  \end{aligned}
  $$
  &lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For any random variable $X$ and a constant $c$:
    &lt;p class=&quot;math&quot; id=&quot;eqn4&quot;&gt;
  $$
  \tag{4} \begin{aligned}
    \mathbb{E}[cX] = \mathbb{E}[c]\mathbb{E}[X] = c\mathbb{E}[X]
  \end{aligned}
  $$
  &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;variance-of-a-random-variable&quot;&gt;Variance of a random variable&lt;/h4&gt;
&lt;p&gt;The variance of a random variable $X$ is the expectation of the squared difference of the random variable from its expectation $\color{blue} \mathbb{E}[X]$. In other words, it measures on average how spread out observations of the random variable are from the expectation of the random variable.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn5&quot;&gt;
  $$
  \begin{aligned}
  \tag{5} \text{Var}(X) &amp;amp; =  \mathbb{E}\Big[\Big(X - {\color{blue} \mathbb{E}[X]}\Big)^2\Big] \\
  
  &amp;amp; =  \mathbb{E}\Big[\Big(X - {\color{blue}\mathbb{E}[X]} \Big)\Big(X - {\color{blue} \mathbb{E}[X]}\Big)\Big] \\ 
 
  &amp;amp; =  \mathbb{E}\Big[\Big(X^2 - 2X{\color{blue}\mathbb{E}[X]} + {\color{blue}\mathbb{E}[X]}^2\Big)\Big] 
  \end{aligned}

  $$
&lt;/p&gt;

&lt;p&gt;If we recall that the expectation of a random variable $\color{blue}\mathbb{E}[X]$ is a constant and if we use the properties stated in equations &lt;a class=&quot;link&quot; id=&quot;3&quot;&gt;(3)&lt;/a&gt; and &lt;a class=&quot;link&quot; id=&quot;4&quot;&gt;(4)&lt;/a&gt;, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn6&quot;&gt;
$$
\tag{6} \begin{aligned}
\text{Var}(X) &amp;amp; = \mathbb{E}\big[X^2\big] - \mathbb{E}\big[2X{\color{blue}\mathbb{E}[X]}\big] + \mathbb{E}\big[{\color{blue}\mathbb{E}[X]}^2\big] \\

&amp;amp; = \mathbb{E}[X^2] - 2{\color{blue}\mathbb{E}[X]\mathbb{E}[X]} + {\color{blue}\mathbb{E}[X]}^2 \\ 

&amp;amp; = \mathbb{E}[X^2] - 2{\color{blue}\mathbb{E}[X]}^2 + {\color{blue}\mathbb{E}[X]}^2 \\ 

\text{Var}(X) &amp;amp; = \mathbb{E}[X^2] - {\color{blue}\mathbb{E}[X]}^2 
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;We could rewrite equation &lt;a class=&quot;link&quot; id=&quot;6&quot;&gt;(6)&lt;/a&gt; as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn7&quot;&gt;
$$
\tag{7} \begin{aligned}
\mathbb{E}[X^2] &amp;amp; = \text{Var}(X) + {\color{blue}\mathbb{E}[X]}^2 \\
&amp;amp; = \mathbb{E}\Big[\Big(X - {\color{blue} \mathbb{E}[X]}\Big)^2\Big] + {\color{blue}\mathbb{E}[X]}^2
\end{aligned}
$$
&lt;/p&gt;

&lt;h3 id=&quot;bias-variance-decomposition-for-regression-problems&quot;&gt;Bias-Variance Decomposition for regression problems&lt;/h3&gt;
&lt;p&gt;If we are trying to predict a quantitative variable $Y$ with features $X$, we may assume that there is a &lt;strong&gt;&lt;em&gt;true, unknown function&lt;/em&gt;&lt;/strong&gt; $\color{green} f(X)$ that defines the relationship between $X$ and $Y$. Linear regression makes the following assumptions about this relationship:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There is a linear function between the conditional population mean of the outcome $Y$ and the features $X$. This function is called the &lt;strong&gt;&lt;em&gt;true regression line&lt;/em&gt;&lt;/strong&gt; and it is the unknown function we will estimate using training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;math&quot; id=&quot;eqn8&quot;&gt;
  $$
    \tag{8} \begin{aligned} 
    \color{green} f(X) = \mathbb{E}[Y|X] = \boldsymbol\beta^\mathsf{T} X
    \end{aligned}
  $$
 &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Individual observations of $Y$ will deviate from the true regression line by a certain amount. For example, if the feature $X$ is age of a person and the outcome variable $Y$ is height, $\color{green}f(X)$ is the mean height of people of a certain age, and each person’s height will deviate from the mean height by a certain amount. An error term $\boldsymbol \epsilon$ captures this deviation. We assume that it is normally distributed with expectation $\mathbb{E}[\boldsymbol \epsilon] = 0$ and variance $\text{Var}(\boldsymbol \epsilon) = \sigma^2$. We also assume that $\boldsymbol \epsilon$ is independent of $X$ and cannot be estimated from data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn9&quot;&gt;
  $$
  \tag{9} \begin{aligned}
  \color{green} Y = f(X) + \epsilon 
  \end{aligned}
  $$
  &lt;/p&gt;

&lt;p&gt;To estimate the true, unknown function, we obtain a training dataset $\mathcal{D}$ of $n$ examples $\big[(x_1, {\color{green}y_1}), \cdots (x_n, {\color{green}y_n})\big]$ drawn i.i.d&lt;sup id=&quot;fnref:iid&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:iid&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; from a data generating distribution $P(X,Y)$ and use a learning algorithm $\mathcal{A}$, say linear regression, to train a model $\color{red}\hat f_\mathcal{D}(X)$ that minimises &lt;strong&gt;&lt;em&gt;mean squared error&lt;/em&gt;&lt;/strong&gt; over the training examples. $\color{red}\hat f_\mathcal{D}(X)$ has subscript $\mathcal{D}$ to indicate that the model was trained on a specific training dataset $\mathcal{D}$. We call $\color{red}\hat f_\mathcal{D}(X)$ the &lt;strong&gt;&lt;em&gt;model&lt;/em&gt;&lt;/strong&gt; of our true, unknown function $\color{green}f(X)$.&lt;/p&gt;

&lt;p&gt;What we really care about is how our model performs on previously unseen test data. For an arbitrary new point $\big(x^\star, \space {\color{green}y^\star = f(x^\star)  + \epsilon} \big)$ drawn from $P(X,Y)$, we can use &lt;strong&gt;&lt;em&gt;squared error&lt;/em&gt;&lt;/strong&gt; to measure the model’s performance on this new example, where $\color{red}\hat f_\mathcal{D}(x^\star)$ is the model’s prediction.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn10&quot;&gt;
$$
\tag{10} \begin{aligned}
\text{Squared Error} = \big({\color{green}y^\star} - {\color{red}\hat f_\mathcal{D}(x^\star)}\big)^2
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Now, because we draw our training set from a data generating distribution, it is possible, in principle&lt;sup id=&quot;fnref:principle&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:principle&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, to randomly draw a large number $N$ of different training datasets &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mo fence=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;D&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;mo&gt;⋯&lt;/mo&gt;&lt;mtext&gt; &lt;/mtext&gt;&lt;mo separator=&quot;true&quot;&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;script&quot;&gt;D&lt;/mi&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/msub&gt;&lt;mo fence=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\big(\mathcal{D}_1, \cdots, \mathcal{D}_N\big)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.20001em;vertical-align:-0.35001em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;delimsizing size1&quot;&gt;(&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathcal&quot; style=&quot;margin-right:0.02778em;&quot;&gt;D&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.30110799999999993em;&quot;&gt;&lt;span style=&quot;top:-2.5500000000000003em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.15em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;minner&quot;&gt;⋯&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mpunct&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.16666666666666666em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathcal&quot; style=&quot;margin-right:0.02778em;&quot;&gt;D&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.32833099999999993em;&quot;&gt;&lt;span style=&quot;top:-2.5500000000000003em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.10903em;&quot;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.15em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;delimsizing size1&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; of $n$  examples from the distribution. If we use the learning algorithm $\mathcal{A}$ to train a model on each training set, we will get $N$ different models $\big({\color{red}\hat f_\mathcal{D_1}(X)}, \cdots, {\color{red}\hat f_\mathcal{D_N}(X)}\big)$ that will give us $N$ different predictions $\big({\color{red}\hat f_\mathcal{D_1}(x^\star)}, \cdots, {\color{red}\hat f_\mathcal{D_N}(x^\star)}\big)$ on our arbitrary new point $x^\star$. We could calculate the &lt;strong&gt;&lt;em&gt;squared error&lt;/em&gt;&lt;/strong&gt; for each of the $N$  models predictions on $x^\star$ $\Big[\big({\color{green}y^\star} - {\color{red}\hat f_\mathcal{D_1}(x^\star)}\big)^2, \cdots, \big({\color{green}y^\star - \color{red}\hat f_\mathcal{D_N}(x^\star)}\big)^2 \big]$. &lt;br /&gt;
To get an idea of how well, on average, the learning algorithm $\mathcal{A}$ generalises to the previously unseen data point, we compute the mean/expectation of the squared errors of the $N$ models. This is called the &lt;strong&gt;&lt;em&gt;expected squared error&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn11&quot;&gt;
$$
\begin{aligned}
\tag{11} \text{Expected Squared Error} = \mathbb{E}\Big[\Big({\color{green}y^\star} - {\color{red}\hat f_\mathcal{D}(x^\star)}\Big)^2\Big]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;It is this expected squared error of the model that we will decompose into the bias, variance, and irreducible error components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/experiment.png&quot; alt=&quot;experiment&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;expectation-bias-and-variance-of-model&quot;&gt;Expectation, Bias, and Variance of model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expectation of model $\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}(x)}\big]$&lt;/strong&gt;&lt;br /&gt;
The expectation of the model is the average of the collection of models estimated over many training datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bias of model&lt;/strong&gt; $\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}(x)}\big] - \color{green}f(x)$&lt;br /&gt;
The model bias describes how much the expectation of the model deviates from the true value of the function $\color{green}f(x)$ we are trying to estimate. Low bias signifies that our model does a good job of approximating our function, and high bias signifies otherwise. The bias measures the &lt;strong&gt;&lt;em&gt;average accuracy&lt;/em&gt;&lt;/strong&gt; of the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Variance of model&lt;/strong&gt; $\mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}(x)} - \mathbb{E}\big[{\color{red}\hat f_\mathcal{D}(x)}\big]\Big)^2\Big]$&lt;br /&gt;
The model variance is the expectation of the squared differences between a particular model and the expectation of the collection of models estimated over many datasets. It captures how much the model fits vary across different datasets, so it measures the &lt;strong&gt;&lt;em&gt;average consistency&lt;/em&gt;&lt;/strong&gt; of the model. A learning algorithm with high variance indicates that the models vary a lot across datasets, while low variance indicates that models are quite similar across datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p class=&quot;tag&quot;&gt;Simulation 1&lt;/p&gt;

&lt;p&gt;In practice, the true function we try to estimate is unknown, but for the sake of demonstration, we will assume that the true function is $f(x) = sin(x)$. Individual observations of $y$ will be $y = sin(x) + \epsilon$. We assume that $\mathbb{E}[\epsilon] = 0$ and $\text{Var}(\epsilon) = \sigma^2 = 1.5$. We will sample 50 data sets each with 100 individual observations, and on them we will fit three polynomial functions of varying degrees/complexities (1, 5, 20) to estimate our true function $f(x)$. Degree 1 is the least complex and degree 20 is the most complex.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/models-complexity.png&quot; alt=&quot;models&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see from the image above that on average, the degree-1 polynomial model does a bad job of estimating our true function. It has a high bias. The variance is low, which means that the model is consistent across datasets. The degree-20 polynomial model has low bias, which means it does a good job of approximating our true function, but it has a high variance. This means that the model isn’t consistent across datasets. The degree-5 polynomial model has low bias (good estimate of our true function), and a relatively low variance (consistent across datasets).&lt;/p&gt;

&lt;p&gt;The code for this simulation can be found &lt;a href=&quot;https://colab.research.google.com/drive/18T2jwJEStq54rNpit3zVXjvjYeplmM4G?usp=sharing&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;a-note-on-bias-variance-tradeoff&quot;&gt;A Note on Bias-Variance Tradeoff&lt;/h4&gt;
&lt;p&gt;Bias-variance tradeoff is the tradeoff in attempting to simultaneously minimise the two sources of error that affect a model’s ability to generalise beyond its training set. Reducing bias generally increases variance and vice versa, and this is a function of a model’s complexity and flexibility. Low variance, high bias models tend to be less complex and less flexible and they mostly underfit the training data, while low bias, high variance models tend to be more complex with a flexible structure that tend to overfit. The optimal model will have both low bias and low variance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/error-complexity.png&quot; alt=&quot;complexity&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;decomposing-expected-squared-error&quot;&gt;Decomposing Expected Squared Error&lt;/h4&gt;
&lt;p&gt;As mentioned earlier, it is the expected squared error term in equation &lt;a class=&quot;link&quot; id=&quot;11&quot;&gt;(11)&lt;/a&gt; that we will decompose into the bias, variance, and irreducible error components. To avoid clutter while decomposing, we will drop the $\color{green}\star$ sign from $\color{green}y^\star$ and $\color{red}(x^\star)$ from $\color{red}\hat f_\mathcal{D}(x^\star)$.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn12&quot;&gt;
$$
\tag{12} \begin{aligned}
\text{Error} &amp;amp; = \mathbb{E}\Big[\Big({\color{green}y} - {\color{red}\hat f_\mathcal{D}}\Big)^2\Big] \\ 

&amp;amp; = \mathbb{E}\Big[\Big({\color{green}y} - {\color{red}\hat f_\mathcal{D}}\Big)\Big({\color{green}y} - {\color{red}\hat f_\mathcal{D}}\Big)\Big] \\

&amp;amp; = \mathbb{E}\Big[ \Big({\color{green}y}^2 - 2{\color{green}y}{\color{red}\hat f_\mathcal{D}} + {\color{red}\hat f_\mathcal{D}}^2 \Big)\Big] \\

&amp;amp; = \underbrace{\mathbb{E}\Big[ {\color{green}y}^2 \Big]}_{1} + \underbrace{\mathbb{E}\Big[ {\color{red}\hat f_\mathcal{D}}^2 \Big]}_{2} - \underbrace{2\mathbb{E}\Big[ {\color{green}y}\Big]\mathbb{E}\Big[ {\color{red}\hat f_\mathcal{D}} \Big]}_{3}

\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Equation &lt;a class=&quot;link&quot; id=&quot;12&quot;&gt;(12)&lt;/a&gt; has three terms that we will deal with separately. If we consider the third term first, we know that $\mathbb{E}\big[ {\color{red}\hat f_\mathcal{D}} \big]$ is the expectation of our model. If we recall equations &lt;a class=&quot;link&quot; id=&quot;2&quot;&gt;(2)&lt;/a&gt; and &lt;a class=&quot;link&quot; id=&quot;9&quot;&gt;(9)&lt;/a&gt; and that $\mathbb{E}[{\boldsymbol\epsilon}] = 0$, we can rewrite $\mathbb{E}\big[ {\color{green}y}\big]$ as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn13&quot;&gt;
$$
\tag{13} \begin{aligned}
\mathbb{E}\Big[ {\color{green}y}\Big] &amp;amp; = \mathbb{E}\Big[ {\color{green}f(x) + \epsilon }\Big] \\
 &amp;amp; = \mathbb{E}\Big[ {\color{green}f(x) }\Big] + \mathbb{E}\Big[ {\color{green} \epsilon }\Big] \\
  &amp;amp; = {\color{green}f(x)}
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;For the first and second terms, we use equations &lt;a class=&quot;link&quot; id=&quot;7&quot;&gt;(7)&lt;/a&gt;, &lt;a class=&quot;link&quot; id=&quot;9&quot;&gt;(9)&lt;/a&gt;, and &lt;a class=&quot;link&quot; id=&quot;13&quot;&gt;(13)&lt;/a&gt; to rewrite them as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn14&quot;&gt;
$$
\tag{14} \begin{aligned}
\mathbb{E}\Big[ {\color{green}y}^2 \Big] &amp;amp; = \mathbb{E}\Big[\Big({\color{green}y} - {\mathbb{E}[{\color{green}y}]}\Big)^2\Big] + {\mathbb{E}[{\color{green}y}]}^2 \\

&amp;amp; = \mathbb{E}\Big[\Big({\color{green}f(x) + \epsilon } - {\color{green}f(x)}\Big)^2\Big] + {\color{green}f(x)}^2 \\

&amp;amp; = \mathbb{E}\Big[{\color{green}\epsilon}^2 \Big] + {\color{green}f(x)}^2 \\

\mathbb{E}\Big[ {\color{red}\hat f_\mathcal{D}}^2 \Big] &amp;amp; = \mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}} - {\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}}\big]}\Big)^2\Big] + {\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}}\big]}^2

\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Substituting equations &lt;a class=&quot;link&quot; id=&quot;13&quot;&gt;(13)&lt;/a&gt; and &lt;a class=&quot;link&quot; id=&quot;14&quot;&gt;(14)&lt;/a&gt; into equation &lt;a class=&quot;link&quot; id=&quot;12&quot;&gt;(12)&lt;/a&gt;, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn15&quot;&gt;
$$
\tag{15} \begin{aligned}
\text{Error} &amp;amp; = \mathbb{E}\Big[ {\color{green}y}^2 \Big] + \mathbb{E}\Big[ {\color{red}\hat f_\mathcal{D}}^2 \Big] - 2\mathbb{E}\Big[ {\color{green}y}\Big]\mathbb{E}\Big[ {\color{red}\hat f_\mathcal{D}} \Big]

\\

&amp;amp; = \mathbb{E}\Big[{\color{green}\epsilon}^2 \Big] + {\color{green}f(x)}^2 + \mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}} - {\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}}\big]}\Big)^2\Big] + {\mathbb{E}\big[{\color{red}\hat f_\mathcal{D}}\big]}^2 - 2{\color{green}f(x)}\mathbb{E}\big[ {\color{red}\hat f_\mathcal{D}} \big]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;If we rearrange equation &lt;a class=&quot;link&quot; id=&quot;15&quot;&gt;(15)&lt;/a&gt; and use expansion of squares $(a-b)^2 = a^2 - 2ab + b^2$, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn16&quot;&gt;
$$
\tag{16} \begin{aligned}
\text{Error} &amp;amp; = \underbrace{ {\mathbb{E}\big[}{\color{red}\hat f_\mathcal{D}} {\big]} ^2 - 2{\color{green}f(x)}\mathbb{E}\big[{\color{red} \hat f_\mathcal{D}}\big] + {\color{green}f(x)}^2 }_{1} + \underbrace{\mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}} - {\mathbb{E}[{\color{red}\hat f_\mathcal{D}}]}\Big)^2\Big]}_{2} + \underbrace{\mathbb{E}\Big[{\color{green}\epsilon}^2 \Big]}_{3} \\

&amp;amp; = \underbrace{\Big( {\mathbb{E}\big[}{\color{red}\hat f_\mathcal{D}} \big] - {\color{green}f(x)} \Big)^2}_{1} + \underbrace{\mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}} - {\mathbb{E}[{\color{red}\hat f_\mathcal{D}}]}\Big)^2\Big]}_{2} + \underbrace{\mathbb{E}\Big[{\color{green}\epsilon}^2 \Big]}_{3} \\

&amp;amp; = \underbrace{\Big( {\mathbb{E}\big[}{\color{red}\hat f_\mathcal{D}} \big] - {\color{green}f(x)} \Big)^2}_{\text{Bias}^2 \space \text{of model}} + \underbrace{\mathbb{E}\Big[\Big({\color{red}\hat f_\mathcal{D}} - {\mathbb{E}[{\color{red}\hat f_\mathcal{D}}]}\Big)^2\Big]}_{\text{Variance of model}} + \underbrace{\sigma^2}_{\text{Irreducible error}}

\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;And there is the error decomposed into its three constituent parts. To see a breakdown of how the third term $\mathbb{E}\Big[{\color{green}\epsilon}^2 \Big]$ became $\sigma^2$, you can check the footnote.&lt;sup id=&quot;fnref:var&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:var&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;illustrating-bias-variance-decomposition-using-simulated-data&quot;&gt;Illustrating Bias-Variance Decomposition using simulated data&lt;/h3&gt;
&lt;p&gt;In Simulation 1, we estimated our true function $f(x) = sin(x)$ by fitting three polynomial functions of varying complexities. What we really care about though is how our models perform on previously unseen data. In this section, we will demonstrate this and show how the expected squared error decomposes into the sum of variance and squared bias by running the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate 100 random datasets of 500 observations each from $sin(x) + \epsilon$. We assume that the $\text{Var}(\epsilon) = \sigma^2 = 1.5$.&lt;/li&gt;
  &lt;li&gt;Split the generated datasets into training and test sets.&lt;/li&gt;
  &lt;li&gt;Fit 20 polynomial functions (degrees from 1 to 20) on each of the training sets.&lt;/li&gt;
  &lt;li&gt;Predict the value of the test set using the fitted models.&lt;/li&gt;
  &lt;li&gt;Calculate the expected prediction error (the mean squared error) on the test set for each model.&lt;/li&gt;
  &lt;li&gt;Show the expected prediction error as a sum of the variance and squared bias.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p class=&quot;tag&quot;&gt;Simulation 2&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bias-variance.png&quot; alt=&quot;bias&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see from graph on the left in the image above that the error starts quite high, drops off to its minimum at model complexity 3, and then starts climbing rapidly as the complexity increases. If we look at the right graph, we see that it follows the bias-variance tradeoff. The squared-bias of the models drops as the complexity increases, but variance increases. Low bias, high variance models suggest that the models overfit the training data and hence performs poorly on the test data. Our optimal model has low bias and low variance and it is the model with complexity 3 in our simulation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/bias-var-sum.png&quot; alt=&quot;Bias-Var&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see from the table above that the mean squared error (MSE) is a sum of the squared bias and the variance, as shown in equation &lt;a class=&quot;link&quot; id=&quot;16&quot;&gt;(16)&lt;/a&gt;. The irreducible part of the decomposition is not added to our sum because we cannot estimate it from data.&lt;/p&gt;

&lt;p&gt;The code for this simulation can be found &lt;a href=&quot;https://colab.research.google.com/drive/1PtUgJGl439xqGvCStOy86p3Q25YlaJHG?usp=sharing&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We have shown the decomposition of the generalisation error for regression problems. It is possible to show this decomposition for classification problems. Pedro Domingos’ brilliant paper, &lt;a href=&quot;https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf&quot;&gt;A Unified Bias-Variance Decomposition and its Application&lt;/a&gt; goes over it in details.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:expectation&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://brilliant.org/wiki/expected-value/&quot; target=&quot;_blank&quot;&gt;Expected value explained on Brilliant.org&lt;/a&gt; &lt;a href=&quot;#fnref:expectation&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:iid&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot; target=&quot;_blank&quot;&gt;Independent and Identically Distributed Random Variables&lt;/a&gt; &lt;a href=&quot;#fnref:iid&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:principle&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In practice, we have access to only one training dataset. &lt;a href=&quot;#fnref:principle&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:var&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Using the variance lemma in equation &lt;a class=&quot;link&quot; id=&quot;7&quot;&gt;(7)&lt;/a&gt; and the assumption that $\mathbb{E}[\boldsymbol \epsilon] = 0$ and $\text{Var}(\boldsymbol \epsilon) = \sigma^2$: &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mtable rowspacing=&quot;0.24999999999999992em&quot; columnalign=&quot;right left&quot; columnspacing=&quot;0em&quot;&gt;&lt;mtr&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mo fence=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;ϵ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo fence=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;mtd&gt;&lt;mstyle scriptlevel=&quot;0&quot; displaystyle=&quot;true&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;Var&lt;/mtext&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;ϵ&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi mathvariant=&quot;double-struck&quot;&gt;E&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;[&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;ϵ&lt;/mi&gt;&lt;msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;]&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;σ&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mstyle&gt;&lt;/mtd&gt;&lt;/mtr&gt;&lt;/mtable&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\begin{aligned} \mathbb{E}\big[{\boldsymbol \epsilon}^2 \big]  &amp;amp; = \text{Var}(\boldsymbol \epsilon) + \mathbb{E}[\boldsymbol \epsilon]^2 = \sigma^2 \end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.524108em;vertical-align:-0.512054em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mtable&quot;&gt;&lt;span class=&quot;col-align-r&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.012054em;&quot;&gt;&lt;span style=&quot;top:-3.147946em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathbb&quot;&gt;E&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;delimsizing size1&quot;&gt;[&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord boldsymbol&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8641079999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;delimsizing size1&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.512054em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;col-align-l&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.012054em;&quot;&gt;&lt;span style=&quot;top:-3.147946em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord text&quot;&gt;&lt;span class=&quot;mord&quot;&gt;Var&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord boldsymbol&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathbb&quot;&gt;E&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord boldsymbol&quot;&gt;ϵ&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;&lt;span class=&quot;mclose&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8641079999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.03588em;&quot;&gt;σ&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.8641079999999999em;&quot;&gt;&lt;span style=&quot;top:-3.113em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.512054em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. &lt;a href=&quot;#fnref:var&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate>
        <link>http://allenkunle.me//bias-variance-decomposition</link>
        <guid isPermaLink="true">http://allenkunle.me//bias-variance-decomposition</guid>
        
        <category>Machine-Learning</category>
        
        <category>Statistics</category>
        
        
      </item>
    
      <item>
        <title>Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II</title>
        <description>&lt;p&gt;In &lt;a href=&quot;/deriving-ml-cost-functions-part1&quot; target=&quot;_blank&quot;&gt;Part I&lt;/a&gt; of this article, we introduced Maximum Likelihood Estimation (MLE), Likelihood function, and derived Mean Squared Error (MSE) using Maximum likelihood estimation. In this article, we will use Maximum likelihood estimation to derive Cross-Entropy cost function, which is commonly used for binary classification problems.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Binary logistic regression is used to model the relationship between a categorical target variable $Y$ and a predictor vector $X = (X_1, X_2, \cdots, X_p)$. The target variable will have two possible values, such as whether a student passes an exam or not, or whether a visitor to a website subscribes to the website’s newsletter or not. The two possible categories are coded as ‘1’, called the positive class, and ‘0’, called the negative class. Binary logistic regression estimates the probability that the response variable $Y$ belongs to the positive class given $X$.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn1&quot;&gt;
$$
\tag{1} \begin{aligned}
  p(X) = Pr(Y = 1 | X)
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;In linear regression, we model the expected value (the mean $\mu$) of the continuous target variable $Y$ as a linear combination of the predictor vector $X$ and estimate the weight parameters $\beta_1, \beta_2, \cdots, \beta_p$ using our training data.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn2&quot;&gt;
$$
\tag{2} \begin{aligned}
  \mathbb{E}(Y|X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;In this case where our target variable $Y$ is categorical and has two possible values coded as 0 and 1, the expected value or mean of $Y$ is the probability $p(X)$ of observing the positive class&lt;sup id=&quot;fnref:expectation&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:expectation&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. It seems sensible then to model the expected value of our categorical $Y$ variable using equation &lt;a class=&quot;link&quot; id=&quot;2&quot;&gt;(2)&lt;/a&gt;, as in linear regression.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn3&quot;&gt;
$$
\tag{3} \begin{aligned}
  \mathbb{E}(Y|X) = p(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The problem with modelling the probability $p(X)$ as a linear combination of the predictor variables is that probability $p(X)$ has a range $[0, 1]$, but the right-hand side of the equation outputs values in the range $(-\infty, +\infty)$. In other words, we will get meaningless estimates of the probability if we use that equation.&lt;br /&gt;
The solution is to use a function of probability $p(X)$ that provides a suitable relationship between the linear combination of the predictor variables $X$ and $p(X)$, the mean of the response variable. This function is called a link function, and it maps the probability range $[0, 1]$ to $(-\infty, +\infty)$.&lt;br /&gt;
The most commonly used link function for binary logistic regression is the logit function (or log-odds&lt;sup id=&quot;fnref:odds&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:odds&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;), given as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn4&quot;&gt;
$$
\tag{4} \begin{aligned}
  \text{logit}\bigg(p(X)\bigg) = \text{log}\bigg(\frac{p(X)}{1-p(X)}\bigg) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;How do we then go from the logit function to getting the estimate of the probability p(X) of observing the positive class? Because logit is a function of probability, we can take its inverse to map arbitrary values in the range $(-\infty, +\infty)$ back to the probability range $[0, 1]$. &lt;br /&gt;
Recall that the inverse function of the natural logarithm function is the exponential function, so if we take the inverse of equation &lt;a class=&quot;link&quot; id=&quot;4&quot;&gt;(4)&lt;/a&gt;, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn5&quot;&gt;
$$
\tag{5} \begin{aligned}
  \frac{p(X)}{1-p(X)} = e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;If we solve for $p(X)$ in equation &lt;a class=&quot;link&quot; id=&quot;5&quot;&gt;(5)&lt;/a&gt;, we get&lt;sup id=&quot;fnref:logistic&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:logistic&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn6&quot;&gt;
$$
\tag{6} \begin{aligned}
  p(X) &amp;amp; = \frac{e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}}{e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)} + 1} \\
      &amp;amp; =  \frac{1}{1 + e^{(-\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}}
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Equation &lt;a class=&quot;link&quot; id=&quot;6&quot;&gt;(6)&lt;/a&gt; is the logistic (or sigmoid) function, and it maps values in the logit range $(-\infty, +\infty)$ back into the range $[0, 1]$ of probabilities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/log-odds.svg&quot; alt=&quot;log-odds&amp;amp;sigmoid&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;deriving-cost-entropy-using-mle&quot;&gt;Deriving Cost Entropy using MLE&lt;/h3&gt;
&lt;p&gt;Given a set of $n$ training examples $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(n)}, y^{(n)})\}$, binary cross-entropy is given by:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn7&quot;&gt;
$$
\tag{7} \begin{aligned}
  \text{Cross-Entropy} = -\bigg[\frac{1}{n} \sum_{i=1}^n\bigg(y^{(i)}\text{log} p^{(i)} + (1- y^{(i)})\text{log}(1 - p^{(i)})\bigg)\bigg]
\end{aligned} 
$$
&lt;/p&gt;

&lt;p&gt;where $x^{(i)}$ is the feature vector, $y^{(i)}$ is the true label (0 or 1) for the $i^{th}$ training example, and $p^{(i)}$ is the predicted probability that the $i^{th}$ training example belongs to the positive class, that is, $Pr(Y = 1 | X = x^{(i)})$.&lt;/p&gt;

&lt;p&gt;In this section, we will derive cross-entropy using MLE. If you are not already familiar with MLE and likelihood function, I will advise that you read the section that explains both concepts in &lt;a href=&quot;/deriving-ml-cost-functions-part1&quot;&gt;Part I&lt;/a&gt; of this article.&lt;/p&gt;

&lt;p&gt;The derivation of cross-entropy follows from using MLE to estimate the parameters $\beta_0, \beta_1, \cdots, \beta_p$ of our logistic model on our training data.&lt;br /&gt;
We start by describing the random process that generated $y^{(i)}$. &lt;br /&gt;
$y^{(i)}$ is a realisation of the Bernoulli random variable&lt;sup id=&quot;fnref:bernoulli&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bernoulli&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; $Y$. The Bernoulli distribution is parameterised by $p$, and its probability mass function (pmf) is given by:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn8&quot;&gt;
$$
\tag{8} \begin{aligned}
  Pr(Y = y^{(i)}) = \begin{cases}
   p &amp;amp; \text{if } y^{(i)} = 1, \\
   1-p &amp;amp; \text {if } y^{(i)} = 0.
 \end{cases}
\end{aligned} 
$$
&lt;/p&gt;

&lt;p&gt;which can be written in the more compact form:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn9&quot;&gt;
$$
\tag{9} \begin{aligned}
  Pr(Y = y^{(i)}) = p^{y^{(i)}}(1-p)^{1 - y^{(i)}} \ \text{for} \ y^{(i)} \in \{0,1\}
\end{aligned} 
$$
&lt;/p&gt;

&lt;p&gt;We then define our Likelihood function. The estimates of $\beta_0, \beta_1, \cdots, \beta_p$ we choose will be the ones that maximise the likelihood function. The likelihood function is a function of our parameter $p$ given our training data:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn10&quot;&gt;
$$
\tag{10} \begin{aligned}
  \mathcal{L}(p | (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(n)}, y^{(n)})) &amp;amp; = \prod_{i=1}^n f(y^{(i)}|p) \\
                                                     &amp;amp; = \prod_{i=1}^n p^{y^{(i)}}(1-p)^{1 - y^{(i)}}
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;It is easier to work with the log of the likelihood function&lt;sup id=&quot;fnref:part1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:part1&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, called the log-likelihood, so if we take the natural logarithm of equation (10), we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn11&quot;&gt;
$$
\tag{11} \begin{aligned}
  \log \bigg(\mathcal{L}(p | y^{(1)}, y^{(2)}, \cdots, y^{(n)})\bigg) &amp;amp; = \log\bigg( \prod_{i=1}^n p^{y^{(i)}}(1-p)^{1 - y^{(i)}} \bigg) \\
    &amp;amp; = \sum_{i=1}^n \log \bigg(p^{y^{(i)}}(1-p)^{1 - y^{(i)}}\bigg) \\
    &amp;amp; = \sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg)
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Recall that for our training data, $p^{(i)}$ in equation &lt;a class=&quot;link&quot; id=&quot;11&quot;&gt;(11)&lt;/a&gt; is the predicted probability of the $i^{th}$ training example gotten from the logisitic function, so it is a function of the parameters $\beta_0, \beta_1, \cdots, \beta_p$. The maximum likelihood estimate $\hat \beta$ is therefore the value of the parameters that maximises the log-likelihood function.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn12&quot;&gt;
$$
\tag{12} \begin{aligned}
  \hat \beta = \underset{\beta}{\operatorname{arg\,max}} \bigg[ \sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \bigg]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;We also know that maximising a function is the same as minimising its negative.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn13&quot;&gt;
$$
\tag{13} \begin{aligned}
  \hat \beta = \underset{\beta}{\operatorname{arg\,min}} \bigg[ -\sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \bigg]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Taking the average across our $n$ training examples, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn14&quot;&gt;
$$
\tag{14} \begin{aligned}
  \hat \beta = \underset{\beta}{\operatorname{arg\,min}} \bigg[\color{red}-\frac{1}{n}\sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \color{black}\bigg]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;which is the cross-entropy as defined in equation &lt;a class=&quot;link&quot; id=&quot;7&quot;&gt;(7)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:expectation&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;$Y$ is a Bernoulli random variable. The Bernoulli distribution is a discrete probability distribution that describes processes that have only two possible outcomes: 1, with a probability of $p$ and 0, with a probability of $1 - p$. The expectation (or mean) of the Bernoulli distribution is $p$. [&lt;a href=&quot;https://brilliant.org/wiki/bernoulli-distribution/&quot; target=&quot;_blank&quot;&gt;Proof]&lt;/a&gt; &lt;a href=&quot;#fnref:expectation&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:odds&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The odds is defined as the ratio of the probability $p$ of observing an event to the probability $1-p$ of not observing that event. $\text{odds} = \frac{p}{1-p}$. If, for example, the odds of an event happening is $o$:1, it means that the event happened $o$ times out of a total of $o+1$ occurrences. The log-odds is simply the natural logarithm of the odds. &lt;a href=&quot;#fnref:odds&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:logistic&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A simple derivation can be found &lt;a href=&quot;https://qr.ae/TWTpnk&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; &lt;a href=&quot;#fnref:logistic&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bernoulli&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli distribution&lt;/a&gt; is the discrete probability distribution of a random variable that takes on two possible values: 1 with probability $p$ and 0 with probability $1-p$. An experiment modelled by the Bernoulli distribution is called a Bernoull trial. Examples of Bernoulli trials include: tossing a coin (head/tail), playing a game (winning/not winning). &lt;a href=&quot;#fnref:bernoulli&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:part1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The reasons why this is the case is explained clearly in &lt;a href=&quot;/deriving-ml-cost-functions-part1&quot;&gt;Part I&lt;/a&gt;. Check the ‘What is Maximum Likelihood Estimation?’ section. &lt;a href=&quot;#fnref:part1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
        <link>http://allenkunle.me//deriving-ml-cost-functions-part2</link>
        <guid isPermaLink="true">http://allenkunle.me//deriving-ml-cost-functions-part2</guid>
        
        <category>Machine-Learning</category>
        
        
      </item>
    
      <item>
        <title>Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part I</title>
        <description>&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;In supervised machine learning, cost functions are used to measure a trained model’s performance. The most commonly used cost function for regression is the Mean Squared Error (MSE), and Cross Entropy for binary classification problems. For many people, the reasons for choosing these cost functions are not at all clear.&lt;br /&gt;
The purpose of this two-part article is to shed some light on the choice of these cost functions by deriving them using Maximum Likelihood Estimation (MLE). Part I will focus on deriving MSE while &lt;a href=&quot;/deriving-ml-cost-functions-part2&quot; target=&quot;_blank&quot;&gt;Part II&lt;/a&gt; will focus on deriving Cross Entropy.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Given a training set of $n$ examples $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots (x^{(n)}, y^{(n)})$ where $x^{(i)}$ is the feature vector for the $i^{th}$ training example and $y^{(i)}$ is its target, the goal of supervised learning is to learn a model $f: \mathcal{X} \to \mathcal{Y}$, a mapping that given $x \in \mathcal{X}$ outputs a prediction $\hat y \in \mathcal{Y}$. $\mathcal X$ and $\mathcal Y$ are called the input space and output space respectively.&lt;/p&gt;

&lt;p&gt;In order to measure how well the model fits our training data, we define a loss function. For training example $(x^{(i)}, y^{(i)})$, the loss $\mathcal{L(y^{(i)}, \hat y^{(i)})}$ measures how different the model’s prediction $\hat y^{(i)}$ is from the true label or value.
The loss is calculated for all training examples, and its average taken. This value is called the cost function and is given by:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn1&quot;&gt;
$$
\tag{1} \begin{aligned}
  Cost = \frac{1}{n}\sum_{i=1}^n\mathcal{L} \bigg( y^{(i)}, \hat y^{(i)} \bigg)
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The model $f$ usually has some unknown parameter $\theta$ (In general, $\theta$ is a vector of parameters) which we will try to estimate using the training set. We can frame supervised learning as an optimisation problem - that is, we estimate the value of $\theta$ by picking the value that minimises the cost function we chose for our problem. Let us call this parameter estimate $\hat \theta$.&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn2&quot;&gt;
$$
\tag{2} \begin{aligned}
  \hat \theta = \underset{\theta}{\operatorname{arg\,min}} \frac{1}{n}\sum_{i=1}^n\mathcal{L}\bigg(y^{(i)}, \hat y^{(i)}\bigg)
\end{aligned}
$$
&lt;/p&gt;

&lt;h3 id=&quot;what-is-maximum-likelihood-estimation&quot;&gt;What is Maximum Likelihood Estimation?&lt;/h3&gt;
&lt;p&gt;Maximum Likelihood Estimation (MLE) is a method of estimating the unknown parameter $\theta$ of a model, given observed data. It estimates the model parameter by finding the parameter value that maximises the likelihood function. The parameter estimate is called the maximum likelihood estimate $\hat{\theta}_{MLE}$.&lt;/p&gt;

&lt;h4 id=&quot;likelihood-function&quot;&gt;Likelihood Function&lt;/h4&gt;
&lt;p&gt;Given a set of independent and identically distributed (i.i.d)&lt;sup id=&quot;fnref:iid&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:iid&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; random variables $X_1, X_2, \cdots, X_n$ from a probability distribution $P_\theta$ with parameter $\theta$. We assume the random variables have a joint probability density function (or joint probability mass function in the case of discrete variables)&lt;sup id=&quot;fnref:jpdf&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:jpdf&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; $f(x_1, x_2,\cdots, x_n|\theta)$. Suppose $x_1, x_2,\cdots, x_n$ are the observed values of the random variables, we define the likelihood function, a function of parameter $\theta$ as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn3&quot;&gt;
$$
\tag{3} \begin{aligned}
  \mathcal{L}(\theta | x_1, x_2, \cdots, x_n) = f(x_1, x_2, \cdots, x_n|\theta) 
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The likelihood function measures how plausible it is that the observed data was generated by the model with a particular value of $\theta$.
For example, if we use $\theta_1$ and $\theta_2$ as values of $\theta$ and find that $\mathcal{L(\theta_1 | x_1, \cdots, x_n)}$ &amp;gt; $\mathcal{L(\theta_2 | x_1, \cdots, x_n)}$, we can reasonably conclude that the observed data is more likely to have been generated by the model with its parameter being $\theta_1$.&lt;/p&gt;

&lt;p&gt;Remember that we assumed that the observed data $x_1, x_2,\cdots, x_n$ were drawn i.i.d from the probability distribution. Also recall that for independent random variables $X_1$ and $X_2$, $f(x_1,x_2|\theta) = f(x_1|\theta) \cdot f(x_2|\theta)$. Considering these, our likelihood function in equation &lt;a class=&quot;link&quot; id=&quot;3&quot;&gt;(3)&lt;/a&gt; becomes:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn4&quot;&gt;
$$
\tag{4} \begin{aligned}
  \mathcal{L(\theta | x_1, x_2, \cdots, x_n)} &amp;amp; = f(x_1|\theta)\cdot f(x_2|\theta)\cdots\cdot f(x_n|\theta) \\
                                          &amp;amp; = \prod_{i=1}^n f(x_i|\theta)
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;where $f(x_i|\theta)$ is the probability density (or mass for discrete variables) function of the random variable $X_i$. Since all the random variables are drawn from the same distribution, their probability density (or mass) function will be the same.&lt;/p&gt;

&lt;p&gt;It is often easier to work with the natural logarithm of the likelihood function, called the log-likelihood. Recalling the product rule of logarithms, $log(a \cdot b) = log(a) + log(b)$. If we apply the product rule to equation &lt;a class=&quot;link&quot; id=&quot;4&quot;&gt;(4)&lt;/a&gt; by taking its natural logarithm, it becomes:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn5&quot;&gt;
$$
\tag{5} \begin{aligned}
  \log \bigg(\mathcal{L(\theta | x_1, x_2, \cdots, x_n)}\bigg) &amp;amp; = \log \bigg( f(x_1|\theta)\cdot f(x_2|\theta)\cdots\cdot f(x_n|\theta) \bigg) \\
                                                              &amp;amp; = \log(f(x_1|\theta)) + \log(f(x_2|\theta)) + \cdots + \log (f(x_n|\theta)) \\
                                                              &amp;amp; = \sum_{i=1}^n \log \bigg(f(x_i|\theta) \bigg)
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;To find the value of $\theta$ that maximises the likelihood function, we find its critical point, the point at which the function’s derivative is $0$. That is:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn6&quot;&gt;
$$
\tag{6} \begin{aligned}
  \frac{\partial \mathcal{L(\theta)}}{\partial \theta} = 0
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;In most cases, this derivative is easier to compute for the log-likelihood function in equation &lt;a class=&quot;link&quot; id=&quot;5&quot;&gt;(5)&lt;/a&gt; than for the vanilla likelihood function in equation «a class=”link” id=”4”&amp;gt;(4)&amp;lt;/a&amp;gt;. Since logarithm is a monotononically increasing function, that is, if $x_1 &amp;gt; x_2$, then $\log(x_1) &amp;gt; log(x_2)$, the value that maximises the likelihood is also the value that maximises the log-likelihood function. &lt;br /&gt;
The other advantage of using log-likehood over likelihood is that it avoids numerical precision issues. Likelihoods are very small numbers, and taking the product of small numbers creates even smaller numbers, which can cause &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_underflow&quot; target=&quot;_blank&quot;&gt;arithmetic underflow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In other words,&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn7&quot;&gt;
$$
\tag{7} \begin{aligned}
  \hat{\theta}_{MLE} =  \underset{\theta}{\operatorname{arg\,max}}\ \mathcal L(\theta\,| x_1, \cdots, x_n)  = \underset{\theta}{\operatorname{arg\,max}}\ \log (\mathcal L(\theta\,| x_1, \cdots, x_n))
\end{aligned}
$$
&lt;/p&gt;

&lt;h3 id=&quot;deriving-mean-squared-errormse-using-mle&quot;&gt;Deriving Mean Squared Error(MSE) using MLE&lt;/h3&gt;
&lt;p&gt;Mean Squared Error is the cost function commonly used for regression. Given a set of $n$ traning examples of the form $(x^{(i)}, y^{(i)})$, MSE is given by:&lt;/p&gt;
&lt;p class=&quot;math&quot; id=&quot;eqn8&quot;&gt;
$$
\tag{8} \begin{aligned}
  MSE = \frac{1}{n} \sum_{i=1}^n\bigg(y^{(i)} - \hat y^{(i)}\bigg)^2
\end{aligned} 
$$
&lt;/p&gt;

&lt;p&gt;where $x^{(i)}$ is the feature vector, $y^{(i)}$ is the true output value, and $\hat y^{(i)}$ is the regression model’s prediction for the $i^{th}$ training example.&lt;/p&gt;

&lt;p&gt;In this section, we will derive MSE using maximum likelihood estimation.&lt;/p&gt;

&lt;h4 id=&quot;regression-model&quot;&gt;Regression Model&lt;/h4&gt;
&lt;p&gt;Given a vector of predictor variables $X = (X_1, X_2, \cdots X_p)$, and quantitative outcome variable $Y$, linear regression assumes that there is a linear relationship between the population mean of the outcome and the predictor variables. This relationship can be expressed by:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn9&quot;&gt;
$$
\tag{9} \begin{aligned}
  \mathbb{E}(Y|X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;$\mathbb{E}(Y|X)$ measures the population mean of $Y$ given a particular value of $X$ and it is often called the true regression line. Now, our observed data will not lie exactly on this true regression line. They will be spread about the true regression line. For example, if we are trying to predict height from age of a population, we will find that people of the same age will have different heights. Each person's height will differ from the population mean $\mathbb{E}(Y|X)$ by a certain amount. We represent this mathematically as:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn10&quot;&gt;
$$
\tag{10} \begin{aligned}
  Y &amp;amp; = \mathbb{E}(Y|X) + \epsilon \\
    &amp;amp; = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The spread about the true regression line is what the $\epsilon$ term captures. We assume it is independent of $X$ and is drawn from a Normal distribution with zero mean ($\mu$ = 0) and variance $\sigma^2$, i.e. $\epsilon \sim \mathcal{N}(0, \sigma^2)$.&lt;/p&gt;

&lt;h4 id=&quot;estimating-linear-regressions-model-parameters&quot;&gt;Estimating linear regression’s model parameters&lt;/h4&gt;
&lt;p&gt;The true regression line and its model parameters $\beta_0, \beta_1, \cdots \beta_p$ in equation &lt;a class=&quot;link&quot; id=&quot;10&quot;&gt;(10)&lt;/a&gt; are unknown, so we will try to estimate them using training data $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots (x^{(n)}, y^{(n)})$, where $x^{(i)}$ is a vector of $p$ predictor variables $(x_1, x_2, \cdots, x_p)$ and $y^{(i)}$ is the target value. Our model (the estimate of the true regression line) is:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn11&quot;&gt;
$$
\tag{11} \begin{aligned}
  y^{(i)} &amp;amp; = \beta_0 + \beta_1 x^{(i)}_1 + \cdots + \beta_p x^{(i)}_p + \epsilon \\
               &amp;amp; = \beta^\intercal x^{(i)} + \epsilon \\
               &amp;amp; = \hat y^{(i)} + \epsilon
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;You will notice that $\hat y^{(i)}$ in equation &lt;a class=&quot;link&quot; id=&quot;11&quot;&gt;(11)&lt;/a&gt; is the estimate of $\mathbb{E}(Y|X)$ in equation &lt;a class=&quot;link&quot; id=&quot;10&quot;&gt;(10)&lt;/a&gt;. The $\epsilon$ term is called the residual and it measures the difference between the observed value $y^{(i)}$ and the predicted value $\hat y^{(i)}$ from the regression equation.&lt;/p&gt;

&lt;p&gt;Estimating $\beta_0, \beta_1, \cdots \beta_p$ using the training data is an optimisation problem that we can solve using MLE by defining a likelihood function. Since $\epsilon$ is normally distributed $\epsilon \sim \mathcal{N}(0, \sigma^2)$, our outcome variable $y$ will also be normally distributed. So, $y \sim \mathcal{N}(\beta^\intercal x, \sigma^2)$  The probability density function of the normal distribution (parameterised by $\mu$: mean, and $\sigma^2$: variance) is given by:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn12&quot;&gt;
$$
\tag{12} \begin{aligned}
  f(y \mid \color{red}\mu, \color{black}\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } e^{ -\frac{(y-\color{red}\mu\color{black})^2}{2\sigma^2} }
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Therefore, if we replace the parameter $\mu$ with the mean of $y$, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn13&quot;&gt;
$$
\tag{13} \begin{aligned}
  f(y \mid \color{red}\beta^\intercal x \color{black}, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } e^{ -\frac{(y-\color{red}\beta^\intercal x\color{black})^2}{2\sigma^2} } 
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The log-likelihood function will then be:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn14&quot;&gt;
$$
\tag{14} \begin{aligned}
  \log \bigg(\mathcal{L(\beta | x^{(1)}, x^{(2)}, \cdots, x^{(n)})}\bigg) &amp;amp; = \sum_{i=1}^n \log \bigg( \frac{1}{\sqrt{2\pi\sigma^2} } e^{ -\frac{(y-\beta^\intercal x^{(i)})^2}{2\sigma^2} }  \bigg) \\
                                                              &amp;amp; = n \log \bigg( \frac{1}{\sqrt{2\pi\sigma^2} } \bigg) - \sum_{i=1}^n \frac{(y-\beta^\intercal x^{(i)})^2}{2\sigma^2}
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Since we need to take the derivative of log-likehood function with respect to $\beta$ to find the maximum likehood estimate of $\beta$, we can remove all the terms that do not contain our parameter $\beta$ as they do not have any effect on our optimisation &lt;sup id=&quot;fnref:remove-terms&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:remove-terms&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, so our equation becomes:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn15&quot;&gt;
$$
\tag{15} \begin{aligned}
  \log \bigg(\mathcal{L(\beta | x^{(1)}, x^{(2)}, \cdots, x^{(n)})}\bigg) &amp;amp; = - \sum_{i=1}^n (y-\beta^\intercal x^{(i)})^2 \\
                                                              &amp;amp; = - \sum_{i=1}^n (y-\hat y^{(i)})^2
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;The value of $\beta$ that maximises equation &lt;a class=&quot;link&quot; id=&quot;15&quot;&gt;(15)&lt;/a&gt; is the maximum likelihood estimate $\hat \beta_{MSE}$. That is,&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn16&quot;&gt;
$$
\tag{16} \begin{aligned}
  \hat \beta_{MSE} = \underset{\beta}{\operatorname{arg\,max}} \bigg[ -\sum_{i=1}^n(y-\hat y^{(i)})^2 \bigg]
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Recall that maximising a function is the same as minimising its negative, so we can rewrite equation &lt;a class=&quot;link&quot; id=&quot;16&quot;&gt;(16)&lt;/a&gt; as&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn17&quot;&gt;
$$
\tag{17} \begin{aligned}
  \hat \beta_{MSE} &amp;amp; = \underset{\beta}{\operatorname{arg\,max}} \bigg[ -\sum_{i=1}^n(y-\hat y^{(i)})^2 \bigg] \\
                   &amp;amp; = \underset{\beta}{\operatorname{arg\,min}} \color{red}\sum_{i=1}^n(y-\hat y^{(i)})^2 
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;Taking the average across all $n$ training examples, we get:&lt;/p&gt;

&lt;p class=&quot;math&quot; id=&quot;eqn18&quot;&gt;
$$
\tag{18} \begin{aligned}
  \hat \beta_{MSE} = \underset{\beta}{\operatorname{arg\,min}} \color{red} \frac{1}{n} \sum_{i=1}^n(y-\hat y^{(i)})^2
\end{aligned}
$$
&lt;/p&gt;

&lt;p&gt;which is exactly the mean squared error (MSE) cost function as defined in equation &lt;a class=&quot;link&quot; id=&quot;8&quot;&gt;(8)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This article introduced the concept of maximum likehood estimation, likehood function, and showed the derivation of mean squared error (MSE). The second part will cover the derivation of cross entropy using maximum likelihood estimation.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:iid&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A set of random variables $X_1, X_2, \cdots, X_n$ are said to be independent and identically distributed if they have the same probability distribution and are mutually independent. &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot; target=&quot;_blank&quot;&gt;Wikipedia Article&lt;/a&gt; &lt;a href=&quot;#fnref:iid&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:jpdf&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading7a.pdf&quot; target=&quot;_blank&quot;&gt;note&lt;/a&gt; from the &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/&quot; target=&quot;_blank&quot;&gt;Introduction to Probability and Statistics&lt;/a&gt; class on MIT OpenCourseWare explains joint probability mass and density functions clearly. &lt;a href=&quot;#fnref:jpdf&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:remove-terms&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The terms without the parameter $\beta$ are regarded as constants in the log-likelihood function, and differentiating a constant gives us zero. &lt;a href=&quot;#fnref:remove-terms&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 24 Mar 2019 00:00:00 +0000</pubDate>
        <link>http://allenkunle.me//deriving-ml-cost-functions-part1</link>
        <guid isPermaLink="true">http://allenkunle.me//deriving-ml-cost-functions-part1</guid>
        
        <category>Machine-Learning</category>
        
        
      </item>
    
      <item>
        <title>dplyr-style Data Manipulation with Pipes in Python</title>
        <description>&lt;p&gt;I often use R’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt; package for exploratory data analysis and data manipulation. In addition to providing a consistent set of functions that one can use to solve the most common data manipulation problems, dplyr also allows one to write elegant, chainable data manipulation code using pipes.&lt;/p&gt;

&lt;p&gt;Now, Python is my main language and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt; is my swiss army knife for data analysis, yet I often wished there was a Python package that allowed dplyr-style data manipulation directly on pandas DataFrames. I searched the Internet and found a package called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt;, developed by &lt;a href=&quot;https://github.com/kieferk&quot; target=&quot;_blank&quot;&gt;Kiefer Katovich&lt;/a&gt;. Like dplyr, dfply also allows chaining of multiple operations with pipe operators.&lt;/p&gt;

&lt;p&gt;This post will focus on the core functions of the dfply package and show how to use them to manipulate pandas DataFrames. The complete source code and dataset is available on &lt;a href=&quot;https://github.com/allenakinkunle/dplyr-style-data-manipulation-in-python&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h3&gt;
&lt;p&gt;The first thing we need to do is install the package using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;dfply&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;According to the project’s Github repo, dfply only works with Python 3, so ensure you have the right version of Python installed.&lt;/p&gt;

&lt;h4 id=&quot;data&quot;&gt;Data&lt;/h4&gt;
&lt;p&gt;To explore the functionality of dfply, we will use the same data used by the &lt;a href=&quot;https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html&quot; target=&quot;_blank&quot;&gt;Introduction to dplyr&lt;/a&gt; vignette. The data is from the &lt;a href=&quot;https://www.bts.gov&quot; target=&quot;_blank&quot;&gt;Bureau of Transporation Statistics&lt;/a&gt; and it contains information about all the 336,776 flights that departed from New York City in 2013.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dfply&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nycflights13.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;year&lt;/th&gt; &lt;th&gt;month&lt;/th&gt; &lt;th&gt;day&lt;/th&gt; &lt;th&gt;dep_time&lt;/th&gt; &lt;th&gt;sched_dep_time&lt;/th&gt; &lt;th&gt;dep_delay&lt;/th&gt; &lt;th&gt;arr_time&lt;/th&gt; &lt;th&gt;sched_arr_time&lt;/th&gt; &lt;th&gt;arr_delay&lt;/th&gt; &lt;th&gt;carrier&lt;/th&gt; &lt;th&gt;flight&lt;/th&gt; &lt;th&gt;tailnum&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;dest&lt;/th&gt; &lt;th&gt;air_time&lt;/th&gt; &lt;th&gt;distance&lt;/th&gt; &lt;th&gt;hour&lt;/th&gt; &lt;th&gt;minute&lt;/th&gt; &lt;th&gt;time_hour&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;517.0&lt;/td&gt;&lt;td&gt;515&lt;/td&gt;&lt;td&gt;2.0&lt;/td&gt;&lt;td&gt;830.0&lt;/td&gt;&lt;td&gt;819&lt;/td&gt;&lt;td&gt;11.0&lt;/td&gt;&lt;td&gt;UA&lt;/td&gt;&lt;td&gt;1545&lt;/td&gt;&lt;td&gt;N14228&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;227.0&lt;/td&gt;&lt;td&gt;1400&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;533.0&lt;/td&gt;&lt;td&gt;529&lt;/td&gt;&lt;td&gt;4.0&lt;/td&gt;&lt;td&gt;850.0&lt;/td&gt;&lt;td&gt;830&lt;/td&gt;&lt;td&gt;20.0&lt;/td&gt;&lt;td&gt;UA&lt;/td&gt;&lt;td&gt;1714&lt;/td&gt;&lt;td&gt;N24211&lt;/td&gt;&lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;227.0&lt;/td&gt;&lt;td&gt;1416&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;29&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;542.0&lt;/td&gt;&lt;td&gt;540&lt;/td&gt;&lt;td&gt;2.0&lt;/td&gt;&lt;td&gt;923.0&lt;/td&gt;&lt;td&gt;850&lt;/td&gt;&lt;td&gt;33.0&lt;/td&gt;&lt;td&gt;AA&lt;/td&gt;&lt;td&gt;1141&lt;/td&gt;&lt;td&gt;N619AA&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;MIA&lt;/td&gt;&lt;td&gt;160.0&lt;/td&gt;&lt;td&gt;1089&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;544.0&lt;/td&gt;&lt;td&gt;545&lt;/td&gt;&lt;td&gt;-1.0&lt;/td&gt;&lt;td&gt;1004.0&lt;/td&gt;&lt;td&gt;1022&lt;/td&gt;&lt;td&gt;-18.0&lt;/td&gt;&lt;td&gt;B6&lt;/td&gt;&lt;td&gt;725&lt;/td&gt;&lt;td&gt;N804JB&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;BQN&lt;/td&gt;&lt;td&gt;183.0&lt;/td&gt;&lt;td&gt;1576&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;554.0&lt;/td&gt;&lt;td&gt;600&lt;/td&gt;&lt;td&gt;-6.0&lt;/td&gt;&lt;td&gt;812.0&lt;/td&gt;&lt;td&gt;837&lt;/td&gt;&lt;td&gt;-25.0&lt;/td&gt;&lt;td&gt;DL&lt;/td&gt;&lt;td&gt;461&lt;/td&gt;&lt;td&gt;N668DN&lt;/td&gt;&lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;ATL&lt;/td&gt;&lt;td&gt;116.0&lt;/td&gt;&lt;td&gt;762&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-01 06:00:00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;piping&quot;&gt;Piping&lt;/h3&gt;
&lt;p&gt;Let’s say you want to perform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; discrete transformation operations on your dataset before outputting the final result. The most common way is to perform the operations step by step and store the result of each step in a variable. The variable holding the intermediate result is then used in the next step of the transformation pipeline. Let’s take a look at an abstract example.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# 'original_data' could be a pandas DataFrame.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformation_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformation_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformation_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;final_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformation_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This isn’t very elegant code and it can get confusing and messy to write. This is where piping comes to the rescue. Piping allows us to rewrite the above code without needing those intermediate variables.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;final_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transformation_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transformation_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transformation_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&amp;gt;&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transformation_n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Magic?! No, it isn’t. Piping works by implicitly making the output of one stage the input of the following stage. In other words, each transformation step works on the transformed result of its previous step.&lt;/p&gt;

&lt;h4 id=&quot;piping-with-dfply&quot;&gt;Piping with dfply&lt;/h4&gt;
&lt;p&gt;dfply allows chaining multiple operations on a pandas DataFrame with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&amp;gt;&lt;/code&gt; operator. One can chain operations and assign the final output (a pandas DataFrame, since dfply works directly on DataFrames) to a variable. In dfply, the DataFrame result of each step of a chain of operations is represented by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;. &lt;br /&gt;
For example, if you want to select three columns from a DataFrame in a step, drop the third column in the next step, and then show the first three rows of the final dataframe, you could do something like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# 'data' is the original pandas DataFrame
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;third_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;third_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop&lt;/code&gt; are both dfply transformation functions, while &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; represents the result of each transformation step.&lt;/p&gt;

&lt;h3 id=&quot;exploring-some-of-dfplys-transformation-methods&quot;&gt;Exploring some of dfply’s transformation methods&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt; provides a set of functions for selecting and dropping columns, subsetting and filtering rows, grouping data, and reshaping data, to name a few.&lt;/p&gt;

&lt;h4 id=&quot;select-and-drop-columns-with-select-and-drop&quot;&gt;Select and drop columns with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Occassionally, you will work on datasets with a lot of columns, but only a subset of the columns will be of interest; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select()&lt;/code&gt; allows you to select these columns.&lt;br /&gt;
For example, to select the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dest&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hour&lt;/code&gt; columns in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight_data&lt;/code&gt; DataFrame we loaded earlier, we do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;dest&lt;/th&gt; &lt;th&gt;hour&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;MIA&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;BQN&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;ATL&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;drop()&lt;/code&gt; is the inverse of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select()&lt;/code&gt;. It returns all the columns except those passed in as arguments. &lt;br /&gt;
For example, to get all the columns except the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;year&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;month&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;day&lt;/code&gt; columns:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can also drop columns inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select()&lt;/code&gt; method by putting a tilde &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~&lt;/code&gt; in front of the column(s) you wish to drop.&lt;br /&gt;
For example, to select all but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hour&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minute&lt;/code&gt; columns in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flight_data&lt;/code&gt; DataFrame:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;filter-rows-with-mask&quot;&gt;Filter rows with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask()&lt;/code&gt; allows you to select a subset of rows in a pandas DataFrame based on logical criteria. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask()&lt;/code&gt; selects all the rows where the criteria is/are true. &lt;br /&gt;
For example, to select all flights longer than 10 hours that originated from JFK airport on January 1:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;month&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'JFK'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;year&lt;/th&gt; &lt;th&gt;month&lt;/th&gt; &lt;th&gt;day&lt;/th&gt; &lt;th&gt;dep_time&lt;/th&gt; &lt;th&gt;sched_dep_time&lt;/th&gt; &lt;th&gt;dep_delay&lt;/th&gt; &lt;th&gt;arr_time&lt;/th&gt; &lt;th&gt;sched_arr_time&lt;/th&gt; &lt;th&gt;arr_delay&lt;/th&gt; &lt;th&gt;carrier&lt;/th&gt; &lt;th&gt;flight&lt;/th&gt; &lt;th&gt;tailnum&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;dest&lt;/th&gt; &lt;th&gt;air_time&lt;/th&gt; &lt;th&gt;distance&lt;/th&gt; &lt;th&gt;hour&lt;/th&gt; &lt;th&gt;minute&lt;/th&gt; &lt;th&gt;time_hour&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;151&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;848.0&lt;/td&gt;&lt;td&gt;1835&lt;/td&gt;&lt;td&gt;853.0&lt;/td&gt;&lt;td&gt;1001.0&lt;/td&gt;&lt;td&gt;1950&lt;/td&gt;&lt;td&gt;851.0&lt;/td&gt;&lt;td&gt;MQ&lt;/td&gt;&lt;td&gt;3944&lt;/td&gt;&lt;td&gt;N942MQ&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;BWI&lt;/td&gt;&lt;td&gt;41.0&lt;/td&gt;&lt;td&gt;184&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;35&lt;/td&gt;&lt;td&gt;2013-01-01 18:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;258&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1059.0&lt;/td&gt;&lt;td&gt;1100&lt;/td&gt;&lt;td&gt;-1.0&lt;/td&gt;&lt;td&gt;1210.0&lt;/td&gt;&lt;td&gt;1215&lt;/td&gt;&lt;td&gt;-5.0&lt;/td&gt;&lt;td&gt;MQ&lt;/td&gt;&lt;td&gt;3792&lt;/td&gt;&lt;td&gt;N509MQ&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;DCA&lt;/td&gt;&lt;td&gt;50.0&lt;/td&gt;&lt;td&gt;213&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-01 11:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;265&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1111.0&lt;/td&gt;&lt;td&gt;1115&lt;/td&gt;&lt;td&gt;-4.0&lt;/td&gt;&lt;td&gt;1222.0&lt;/td&gt;&lt;td&gt;1226&lt;/td&gt;&lt;td&gt;-4.0&lt;/td&gt;&lt;td&gt;B6&lt;/td&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;N279JB&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;BTV&lt;/td&gt;&lt;td&gt;52.0&lt;/td&gt;&lt;td&gt;266&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;2013-01-01 11:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;266&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1112.0&lt;/td&gt;&lt;td&gt;1100&lt;/td&gt;&lt;td&gt;12.0&lt;/td&gt;&lt;td&gt;1440.0&lt;/td&gt;&lt;td&gt;1438&lt;/td&gt;&lt;td&gt;2.0&lt;/td&gt;&lt;td&gt;UA&lt;/td&gt;&lt;td&gt;285&lt;/td&gt;&lt;td&gt;N517UA&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;SFO&lt;/td&gt;&lt;td&gt;364.0&lt;/td&gt;&lt;td&gt;2586&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-01 11:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;272&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1124.0&lt;/td&gt;&lt;td&gt;1100&lt;/td&gt;&lt;td&gt;24.0&lt;/td&gt;&lt;td&gt;1435.0&lt;/td&gt;&lt;td&gt;1431&lt;/td&gt;&lt;td&gt;4.0&lt;/td&gt;&lt;td&gt;B6&lt;/td&gt;&lt;td&gt;641&lt;/td&gt;&lt;td&gt;N590JB&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;SFO&lt;/td&gt;&lt;td&gt;349.0&lt;/td&gt;&lt;td&gt;2586&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-01 11:00:00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;h4 id=&quot;sort-rows-with-arrange&quot;&gt;Sort rows with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrange()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrange()&lt;/code&gt; allows you to order rows based on one or multiple columns; the default behaviour is to sort the rows in ascending order. &lt;br /&gt;
For example, to sort by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distance&lt;/code&gt; and then by the number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hours&lt;/code&gt; the flights take, we do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;arrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;year&lt;/th&gt; &lt;th&gt;month&lt;/th&gt; &lt;th&gt;day&lt;/th&gt; &lt;th&gt;dep_time&lt;/th&gt; &lt;th&gt;sched_dep_time&lt;/th&gt; &lt;th&gt;dep_delay&lt;/th&gt; &lt;th&gt;arr_time&lt;/th&gt; &lt;th&gt;sched_arr_time&lt;/th&gt; &lt;th&gt;arr_delay&lt;/th&gt; &lt;th&gt;carrier&lt;/th&gt; &lt;th&gt;flight&lt;/th&gt; &lt;th&gt;tailnum&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;dest&lt;/th&gt; &lt;th&gt;air_time&lt;/th&gt; &lt;th&gt;distance&lt;/th&gt; &lt;th&gt;hour&lt;/th&gt; &lt;th&gt;minute&lt;/th&gt; &lt;th&gt;time_hour&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;275945&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;106&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;245&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;US&lt;/td&gt;&lt;td&gt;1632&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;NaN&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2013-07-27 01:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3083&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1240.0&lt;/td&gt;&lt;td&gt;1200&lt;/td&gt;&lt;td&gt;40.0&lt;/td&gt;&lt;td&gt;1333.0&lt;/td&gt;&lt;td&gt;1306&lt;/td&gt;&lt;td&gt;27.0&lt;/td&gt;&lt;td&gt;EV&lt;/td&gt;&lt;td&gt;4193&lt;/td&gt;&lt;td&gt;N14972&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;PHL&lt;/td&gt;&lt;td&gt;30.0&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-04 12:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3901&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1155.0&lt;/td&gt;&lt;td&gt;1200&lt;/td&gt;&lt;td&gt;-5.0&lt;/td&gt;&lt;td&gt;1241.0&lt;/td&gt;&lt;td&gt;1306&lt;/td&gt;&lt;td&gt;-25.0&lt;/td&gt;&lt;td&gt;EV&lt;/td&gt;&lt;td&gt;4193&lt;/td&gt;&lt;td&gt;N14902&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;PHL&lt;/td&gt;&lt;td&gt;29.0&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-05 12:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3426&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1829.0&lt;/td&gt;&lt;td&gt;1615&lt;/td&gt;&lt;td&gt;134.0&lt;/td&gt;&lt;td&gt;1937.0&lt;/td&gt;&lt;td&gt;1721&lt;/td&gt;&lt;td&gt;136.0&lt;/td&gt;&lt;td&gt;EV&lt;/td&gt;&lt;td&gt;4502&lt;/td&gt;&lt;td&gt;N15983&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;PHL&lt;/td&gt;&lt;td&gt;28.0&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;2013-01-04 16:00:00&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;10235&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;1613.0&lt;/td&gt;&lt;td&gt;1617&lt;/td&gt;&lt;td&gt;-4.0&lt;/td&gt;&lt;td&gt;1708.0&lt;/td&gt;&lt;td&gt;1722&lt;/td&gt;&lt;td&gt;-14.0&lt;/td&gt;&lt;td&gt;EV&lt;/td&gt;&lt;td&gt;4616&lt;/td&gt;&lt;td&gt;N11150&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;PHL&lt;/td&gt;&lt;td&gt;36.0&lt;/td&gt;&lt;td&gt;80&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;2013-01-12 16:00:00&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To sort in descending order, you set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ascending&lt;/code&gt; keyword argument of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arrange()&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;, like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;arrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;add-new-columns-with-mutate&quot;&gt;Add new columns with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate()&lt;/code&gt; allows you to create new columns in the DataFrame. The new columns can be composed from existing columns.&lt;br /&gt;
For example, let’s create two new columns: one by dividing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;distance&lt;/code&gt; column by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1000&lt;/code&gt;, and the other by concatenating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;carrier&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;origin&lt;/code&gt; columns. We will name these new columns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new_distance&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;carrier_origin&lt;/code&gt; respectively.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;new_distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;carrier_origin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;carrier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;year&lt;/th&gt; &lt;th&gt;month&lt;/th&gt; &lt;th&gt;day&lt;/th&gt; &lt;th&gt;dep_time&lt;/th&gt; &lt;th&gt;sched_dep_time&lt;/th&gt; &lt;th&gt;dep_delay&lt;/th&gt; &lt;th&gt;arr_time&lt;/th&gt; &lt;th&gt;sched_arr_time&lt;/th&gt; &lt;th&gt;arr_delay&lt;/th&gt; &lt;th&gt;carrier&lt;/th&gt; &lt;th&gt;...&lt;/th&gt; &lt;th&gt;tailnum&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;dest&lt;/th&gt; &lt;th&gt;air_time&lt;/th&gt; &lt;th&gt;distance&lt;/th&gt; &lt;th&gt;hour&lt;/th&gt; &lt;th&gt;minute&lt;/th&gt; &lt;th&gt;time_hour&lt;/th&gt; &lt;th&gt;carrier_origin&lt;/th&gt; &lt;th&gt;new_distance&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;517.0&lt;/td&gt;&lt;td&gt;515&lt;/td&gt;&lt;td&gt;2.0&lt;/td&gt;&lt;td&gt;830.0&lt;/td&gt;&lt;td&gt;819&lt;/td&gt;&lt;td&gt;11.0&lt;/td&gt;&lt;td&gt;UA&lt;/td&gt;&lt;td&gt;...&lt;/td&gt;&lt;td&gt;N14228&lt;/td&gt;&lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;227.0&lt;/td&gt;&lt;td&gt;1400&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;td&gt;UAEWR&lt;/td&gt;&lt;td&gt;1.400&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;533.0&lt;/td&gt;&lt;td&gt;529&lt;/td&gt;&lt;td&gt;4.0&lt;/td&gt;&lt;td&gt;850.0&lt;/td&gt;&lt;td&gt;830&lt;/td&gt;&lt;td&gt;20.0&lt;/td&gt;&lt;td&gt;UA&lt;/td&gt;&lt;td&gt;...&lt;/td&gt;&lt;td&gt;N24211&lt;/td&gt;&lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;IAH&lt;/td&gt;&lt;td&gt;227.0&lt;/td&gt;&lt;td&gt;1416&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;29&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;td&gt;UALGA&lt;/td&gt;&lt;td&gt;1.416&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;542.0&lt;/td&gt;&lt;td&gt;540&lt;/td&gt;&lt;td&gt;2.0&lt;/td&gt;&lt;td&gt;923.0&lt;/td&gt;&lt;td&gt;850&lt;/td&gt;&lt;td&gt;33.0&lt;/td&gt;&lt;td&gt;AA&lt;/td&gt;&lt;td&gt;...&lt;/td&gt;&lt;td&gt;N619AA&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;MIA&lt;/td&gt;&lt;td&gt;160.0&lt;/td&gt;&lt;td&gt;1089&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;td&gt;AAJFK&lt;/td&gt;&lt;td&gt;1.089&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;3&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;544.0&lt;/td&gt;&lt;td&gt;545&lt;/td&gt;&lt;td&gt;-1.0&lt;/td&gt;&lt;td&gt;1004.0&lt;/td&gt;&lt;td&gt;1022&lt;/td&gt;&lt;td&gt;-18.0&lt;/td&gt;&lt;td&gt;B6&lt;/td&gt;&lt;td&gt;...&lt;/td&gt;&lt;td&gt;N804JB&lt;/td&gt;&lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;BQN&lt;/td&gt;&lt;td&gt;183.0&lt;/td&gt;&lt;td&gt;1576&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;2013-01-01 05:00:00&lt;/td&gt;&lt;td&gt;B6JFK&lt;/td&gt;&lt;td&gt;1.576&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;4&lt;/th&gt; &lt;td&gt;2013&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;554.0&lt;/td&gt;&lt;td&gt;600&lt;/td&gt;&lt;td&gt;-6.0&lt;/td&gt;&lt;td&gt;812.0&lt;/td&gt;&lt;td&gt;837&lt;/td&gt;&lt;td&gt;-25.0&lt;/td&gt;&lt;td&gt;DL&lt;/td&gt;&lt;td&gt;...&lt;/td&gt;&lt;td&gt;N668DN&lt;/td&gt;&lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;ATL&lt;/td&gt;&lt;td&gt;116.0&lt;/td&gt;&lt;td&gt;762&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;2013-01-01 06:00:00&lt;/td&gt;&lt;td&gt;DLLGA&lt;/td&gt;&lt;td&gt;0.762&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The newly created columns will be at the end of the DataFrame.&lt;/p&gt;

&lt;h4 id=&quot;group-and-ungroup-data-with-group_by-and-ungroup&quot;&gt;Group and ungroup data with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group_by()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ungroup()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group_by()&lt;/code&gt; allows you to group the DataFrame by one or multiple columns. Functions chained after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group_by()&lt;/code&gt; are applied on the group until the DataFrame is ungrouped with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ungroup()&lt;/code&gt; function. For example, to group the data by the originating airport, we do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;summarise-data-using-summarize&quot;&gt;Summarise data using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summarize()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summarize()&lt;/code&gt; is typically used together with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group_by()&lt;/code&gt; to reduce each group into a single row summary. In other words, the output will have one row for each group. For example, to calculate the mean distance for flights originating from every airport, we do:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;mean_distance&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;1056.742790&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;1266.249077&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;779.835671&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;h3 id=&quot;bringing-it-all-together-with-pipes&quot;&gt;Bringing it all together with pipes&lt;/h3&gt;
&lt;p&gt;Let’s say you want to perform the following operations on the flights data&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;[Step 1]: Filter out all flights less than 10 hours&lt;/li&gt;
  &lt;li&gt;[Step 2]: Create a new column, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speed&lt;/code&gt;, using the formula [distance / (air time * 60)]&lt;/li&gt;
  &lt;li&gt;[Step 3]: Calculate the mean speed for flights originating from each airport&lt;/li&gt;
  &lt;li&gt;[Step 4]: Sort the result by mean speed in descending order&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will write the operations using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt; piping operator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&amp;gt;&lt;/code&gt;. We won’t have to use intermediate variables to save the result of each step.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hour&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step 1
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;air_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step 2
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step 3a
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;summarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_speed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step 3b
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;arrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step 4
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt; &lt;thead&gt; &lt;tr style=&quot;text-align: right;&quot;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;origin&lt;/th&gt; &lt;th&gt;mean_speed&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;0&lt;/th&gt; &lt;td&gt;EWR&lt;/td&gt;&lt;td&gt;0.109777&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;1&lt;/th&gt; &lt;td&gt;JFK&lt;/td&gt;&lt;td&gt;0.109427&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;th&gt;2&lt;/th&gt; &lt;td&gt;LGA&lt;/td&gt;&lt;td&gt;0.107362&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;If we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt; data manipulation functions instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt;’s, our code will look something like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hour'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'speed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'distance'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'air_time'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flight_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'origin'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as_index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'speed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'speed'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I find the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt; version easier to read and understand than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt; version.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This is by no means an exhaustive coverage of the functionality of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dfply&lt;/code&gt; package. The &lt;a href=&quot;https://github.com/kieferk/dfply&quot; target=&quot;_blank&quot;&gt;package documentation&lt;/a&gt; is really good and I advise that you check it out to learn more.&lt;/p&gt;

&lt;p&gt;If you have suggestions or questions, please drop a comment in the comment section below. You can also send me an email at hello [at] allenkunle [dot] me or tweet at me &lt;a href=&quot;https://twitter.com/allenakinkunle&quot; target=&quot;_blank&quot;&gt;@allenakinkunle&lt;/a&gt;, and I will reply as soon as I can.&lt;/p&gt;

&lt;p&gt;The complete source code for this blog post is available on &lt;a href=&quot;https://github.com/allenakinkunle/dplyr-style-data-manipulation-in-python&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;. Thank you for reading, and please don’t forget to share.&lt;/p&gt;

</description>
        <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
        <link>http://allenkunle.me//dplyr-style-data-manipulation-in-python</link>
        <guid isPermaLink="true">http://allenkunle.me//dplyr-style-data-manipulation-in-python</guid>
        
        <category>Python</category>
        
        <category>pandas</category>
        
        <category>dplyr</category>
        
        <category>Data-Wrangling</category>
        
        
      </item>
    
      <item>
        <title>Exploratory Analysis of the Washington's Post Police Shooting dataset using R and Plotly</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://www.washingtonpost.com/graphics/national/police-shootings-2016/&quot; target=&quot;_blank&quot;&gt;Washington Post&lt;/a&gt; has been compiling a database of fatal shootings in the United States by police officers in the line of duty since January 1, 2015. The &lt;a href=&quot;https://github.com/washingtonpost/data-police-shootings&quot; target=&quot;_blank&quot;&gt;dataset&lt;/a&gt; contains information such as the date and the state in which the killing occurred, the age, gender and, race of the deceased.&lt;/p&gt;

&lt;p&gt;In light of the recent Police killings in the US, I thought it would be interesting to perform the following exploratory analyses using the dataset:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The distribution of the deceased by age group&lt;/li&gt;
  &lt;li&gt;The distribution of Police killings per million people for each state using a choropleth map.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will use R and &lt;a href=&quot;https://plot.ly&quot; target=&quot;_blank&quot;&gt;Plotly&lt;/a&gt; for this purpose.&lt;/p&gt;

&lt;h3 id=&quot;required-packages&quot;&gt;Required Packages&lt;/h3&gt;
&lt;p&gt;For this analysis, we will need the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt; package for data manipulation, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;magrittr&lt;/code&gt; package for pipe-like operations, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plotly&lt;/code&gt; package for creating interactive graphs. Install these packages using R’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;install.packages&lt;/code&gt; method if you haven’t already, then load the packages using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;library&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;The complete source code for this post is available on &lt;a href=&quot;https://github.com/allenakinkunle/washington-post-data-analysis&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data Preparation&lt;/h3&gt;
&lt;p&gt;The first thing we need to do is read the data into R as a dataframe from where it is hosted on Github.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/fatal-police-shootings-data.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;colnames()&lt;/code&gt; method to get the column names of the data frame.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;date&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;manner_of_death&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;        
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;armed&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                   &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;age&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                     &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;gender&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;race&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                   
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                   &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;signs_of_mental_illness&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;threat_level&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;           
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;flee&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;body_camera&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We see that there are 14 variables. We only need the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;age&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state&lt;/code&gt; variables for our analysis so we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select()&lt;/code&gt; method to create a new data frame with only those variables selected. Remember that you can always check out a method’s documentation in R using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?method_name&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s see what the new dataframe looks like&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WA&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OR&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KS&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CA&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;39&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CO&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OK&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The age column contains integer values and the state column contains the 2-letter abbreviations for the states in which the killings occurred.&lt;/p&gt;

&lt;h3 id=&quot;handling-missing-values&quot;&gt;Handling Missing Values&lt;/h3&gt;
&lt;p&gt;Before performing any analysis, it’s a good practice to check for anomalies like missing values in the data. Missing values are coded with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NA&lt;/code&gt; in most datasets, so we check for this in our dataset. Be familiar with how missing values are represented in your dataset and handle them accordingly.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;37&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sapply()&lt;/code&gt; method applies the function supplied as its argument to each column of the data frame. The function returns the number of cells that have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NA&lt;/code&gt; as their values in each of the dataframe’s column. To learn more about the Apply family of functions in R, check this &lt;a href=&quot;https://www.datacamp.com/community/tutorials/r-tutorial-apply-family&quot; target=&quot;_blank&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We see that there are 37 missing values coded as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NA&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;age&lt;/code&gt; column so we handle this by recoding them to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;. The choice of 0 will be apparent in the following section.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;how-many-people-are-killed-by-age-group&quot;&gt;How many people are killed by age group?&lt;/h3&gt;
&lt;p&gt;Since the ages are integer values, I thought it would be more useful to group them and then plot the distribution of killings in the age groups. We convert the integer values into categories (factors) using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut()&lt;/code&gt; method. Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?cut&lt;/code&gt; in R, we get the following documentation:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cut&lt;/code&gt; divides the range of x into intervals and codes the values in x according to which interval they fall. The leftmost interval corresponds to level one, the next leftmost to level two and so on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;breaks&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;Inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;Inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Unknown&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Under 18&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;18-29&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;30-44&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;45-59&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;60 above&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since we coded our missing age values as 0, we set the label for any value between -Inf and 1 to &lt;em&gt;“Unknown”&lt;/em&gt;. This interval covers our missing values coded as 0. Ages between 1 and 18 are grouped into &lt;em&gt;“Under 18”&lt;/em&gt;, 18 - 30 coded as &lt;em&gt;“18-29”&lt;/em&gt;, 30 - 45 coded as &lt;em&gt;“30-44”&lt;/em&gt;, 45 - 60 coded as &lt;em&gt;“45 - 59”&lt;/em&gt; while ages between 60 and Inf are coded as &lt;em&gt;“60 above”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the following code snippet, we group the data frame by the age group categories column &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat_age&lt;/code&gt; we created above and then summarise each group by counting the number of killings. We pass the summarised data into ggplot and display it as a bar chart.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;killings_by_age_group_plot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#f29999&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Age Group&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Number of Police Killings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Distribution of Police Killings by age group in the United States (Jan 2015 - July 2016)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplotly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_by_age_group_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Make graph interactive with Plotly&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The reason Plotly was chosen for this analysis is because it allows us to create beautiful, interactive visualisation with APIs in Python, R and many other languages. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ggplotly&lt;/code&gt; method accepts a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ggplot&lt;/code&gt; object as argument and turns it into an interactive graph. Check out &lt;a href=&quot;https://plot.ly/r/#basic-charts&quot; target=&quot;_blank&quot;&gt;some example plots&lt;/a&gt; created with Plotly’s R library.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&amp;gt;%&lt;/code&gt; operator in the snippet above might look strange to some R users. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&amp;gt;%&lt;/code&gt;, available in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;magrittr&lt;/code&gt; package, allows us to pipe values into an expression or a function call. It improves code readability by removing the need to create a bunch of variables to hold the results of function calls. Check out &lt;a href=&quot;https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html&quot; target=&quot;_blank&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;magrittr&lt;/code&gt;’s vignette&lt;/a&gt; for a detailed explanation of how to use the package.&lt;/p&gt;

&lt;div class=&quot;plotly&quot;&gt;
  &lt;iframe frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://plot.ly/~allenkunle/0.embed&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The plot is interactive so hover and click on it to interact with it.&lt;/p&gt;

&lt;h3 id=&quot;what-is-the-police-killings-per-million-population-value-for-each-state&quot;&gt;What is the Police killings per million population value for each state?&lt;/h3&gt;
&lt;p&gt;To compare the number of police killings across US states in our dataset, we will compute the police killings per million population value for each state and plot these values on a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plotly&lt;/code&gt;’s choropleth map.&lt;/p&gt;

&lt;p&gt;For this analysis, we need the population estimates for each state and the full state names. These details are not in the Washington Post’s dataset so we need to get them from other sources. We will use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.abb&lt;/code&gt; (contains the 2-letter state name abbreviations) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.name&lt;/code&gt; (contains the full state names) datasets available in R to get the full state names into our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shooting_data&lt;/code&gt; dataframe.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state&lt;/code&gt; data sets contain information relating to the 50 states of the US and they are arranged according to alphabetical order of the state names. So for an index &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;, the 2-letter state name abbreviation at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.abb[i]&lt;/code&gt; will map to the full state name at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.name[i]&lt;/code&gt;. One caveat of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state&lt;/code&gt; datasets is that they do not contain information about the District of Columbia.&lt;/p&gt;

&lt;p&gt;For the population estimates of each state, I created a state population data set from data I extracted from the &lt;a href=&quot;https://www.census.gov/popest/data/state/totals/2015/index.html&quot; target=&quot;_blank&quot;&gt;United States Census Bureau website&lt;/a&gt;. I have cleaned the data set and it contains 2015 population estimates for each of the 50 US states and the District of Columbia. The data set is hosted &lt;a href=&quot;https://raw.githubusercontent.com/allenakinkunle/washington-post-data-analysis/master/state_population_data.csv&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We start our analysis by adding the full state name for each row containing the 2-letter state name abbreviation in our dataframe. We check for matches between the values of our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shooting_data$state&lt;/code&gt; column and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.abb&lt;/code&gt; vector. If there is a match, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;match()&lt;/code&gt; method returns the index of the match in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state.abb&lt;/code&gt; vector, else it returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NA&lt;/code&gt;, in this case this only happens for &lt;strong&gt;DC&lt;/strong&gt; (District of Columbia) abbreviation.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Get full state name if index exists, else set full state name as &quot;District of Columbia&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state.abb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;District of Columbia&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state.name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s take a look into how the dataframe looks now&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat_age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WA&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;45-59&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Washington&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OR&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;45-59&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Oregon&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;   &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KS&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;18-29&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Kansas&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We see that the full state name has been added to the dataframe. To get the population estimates for each state, we read in the state population data set.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;state_population_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://raw.githubusercontent.com/allenakinkunle/washington-post-data-analysis/master/state_population_data.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In the following code snippet, we group the data frame by state name and count the number of killings in each state. We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;merge()&lt;/code&gt; to perform an inner join of our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shooting_data&lt;/code&gt;  dataframe and newly imported &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state_population_data&lt;/code&gt; dataframe by their common column &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state_name&lt;/code&gt;. We then use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dplyr&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutate&lt;/code&gt; method to add the computed Police killings per million for each state to the dataframe. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hover&lt;/code&gt; variable, also added to the dataframe, is used by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plotly&lt;/code&gt; to display a hover text when the choropleth map is interacted with.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;killings_by_state&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shooting_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summarise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_population_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;state_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# calculate the killings per million for each state&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_per_million&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digits&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hover&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&amp;lt;br&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_per_million&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'per million people'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                  &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&amp;lt;br&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'shootings since January 2015'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next step is to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plotly&lt;/code&gt;’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;plot_ly&lt;/code&gt; method to create the choropleth map. We pass in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;killings_by_state&lt;/code&gt; dataframe we created above as argument to the function among other configuration arguments.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# List of options for the map&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toRGB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;white&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'usa'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Plot the choropleth map&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_ly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_by_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_per_million&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;locations&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'choropleth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;locationmode&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'USA-states'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;killings_per_million&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Reds'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;marker&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colorbar&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Police killings per million people&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lenmode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pixels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;titleside&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;right&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ypad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&amp;gt;%&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Police killings per million people in United States (Jan 2015 - July 2016))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geo&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;plotly&quot;&gt;
  &lt;iframe frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://plot.ly/~allenkunle/2.embed&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Hover on the map, zoom in and out to interact with it.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This blog post shows the power of R for quick exploratory analysis and demonstrates how static graphs can be brought to life with Plotly. Plotly is easy to use for users familiar with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ggplot&lt;/code&gt; as it uses a similar syntax. If you have suggestions or questions, please drop a comment in the comment section below. You can  also send me emails at hello [at] allenkunle [dot] me or tweet at me &lt;a href=&quot;https://twitter.com/allenakinkunle&quot; target=&quot;_blank&quot;&gt;@allenakinkunle&lt;/a&gt;, and I will reply as soon as I can.&lt;/p&gt;

&lt;p&gt;The complete source code is available on &lt;a href=&quot;https://github.com/allenakinkunle/washington-post-data-analysis&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;. Feel free to check and make suggestions for improvements. Thank you for reading!&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jul 2016 00:00:00 +0100</pubDate>
        <link>http://allenkunle.me//exploratory-analysis-police-shooting</link>
        <guid isPermaLink="true">http://allenkunle.me//exploratory-analysis-police-shooting</guid>
        
        <category>R</category>
        
        <category>Plotly</category>
        
        
      </item>
    
  </channel>
</rss>
