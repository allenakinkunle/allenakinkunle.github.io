<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Data Science, Machine Learning, Data Visualisation, Software Development">

	<title>Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II</title>

	<!-- Twitter cards -->
	<meta name="twitter:site"    content="@allenakinkunle">
	<meta name="twitter:creator" content="@allenakinkunle">
	<meta name="twitter:title"   content="Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II">
	<meta name="twitter:card"    content="summary_large_image">
	<meta name="twitter:image"   content="http://allenkunle.me/img/mathematics.jpg">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">
	<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
	<script>
	WebFont.load({
		google: {
			families: [
				'Crimson Text:400,600',
				'Source Sans Pro:600',
				'Signika:600'
			]
		}
	});
	</script>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

	<!-- Feed -->
	<link rel="alternate" type="application/rss+xml" title="My Site RSS" href="/feed.xml" />

	<!-- Icons -->
	<link rel="shortcut icon" href="/img/favicon.ico">

	<!-- Google Analytics -->
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-136832785-1', 'auto');
  ga('send', 'pageview');

</script>


	<!-- Katex-->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
		onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            'delimiters': [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true}
            ]
        });
    });
</script>
	
</head>


<body>

	<div id="wrap">

		<!-- Header -->
		<header id="header">
	<div class="container">
		<a href="/">
		  <h1>Allen Akinkunle</h1>
		</a>

		<div class="social">
		  <a href="https://github.com/allenakinkunle" target="_blank"><i class="fa fa-github"></i></a> <i class="dot">.</i>
		  <a href="mailto:hello@allenkunle.me" target="_blank"><i class="fa fa-envelope"></i></a> <i class="dot">.</i>
		  <a href="http://twitter.com/allenakinkunle" target="_blank"><i class="fa fa-twitter"></i></a> <i class="dot">.</i>
		  <a href="http://uk.linkedin.com/in/allenkunle" target="_blank"><i class="fa fa-linkedin"></i></a> <i class="dot">.</i>
			<a href="http://stackoverflow.com/users/1748587/akinkunle-allen" target="_blank"><i class="fa fa-stack-overflow"></i></a> <i class="dot">.</i>
			<a href="/about/" class="about">About</a>
		</div>
	</div>
</header>


    <!-- Main content -->
	  <div class="container">

		<main>

			<article id="post-page">
	<h2 class="title">Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II</h2>

	<div id="metadata">
	  <time datetime="2019-03-31T00:00:00+00:00" class="time"><i class="fa fa-clock-o"></i>31 Mar 2019</time>
	  <p class="tags"><i class="fa fa-tags"></i>
	    
	      <a href="/tags/Machine-Learning">Machine-Learning</a>
	    
	  </p>
	</div>

	<div class="content">

		<p>In <a href="/deriving-ml-cost-functions-part1" target="_blank">Part I</a> of this article, we introduced Maximum Likelihood Estimation (MLE), Likelihood function, and derived Mean Squared Error (MSE) using Maximum likelihood estimation. In this article, we will use Maximum likelihood estimation to derive Cross-Entropy cost function, which is commonly used for binary classification problems.</p>

<h3 id="background">Background</h3>
<p>Binary logistic regression is used to model the relationship between a categorical target variable $Y$ and a predictor vector $X = (X_1, X_2, \cdots, X_p)$. The target variable will have two possible values, such as whether a student passes an exam or not, or whether a visitor to a website subscribes to the website’s newsletter or not. The two possible categories are coded as ‘1’, called the positive class, and ‘0’, called the negative class. Binary logistic regression estimates the probability that the response variable $Y$ belongs to the positive class given $X$.</p>

<p class="math">
$$
\begin{align}
  p(X) = Pr(Y = 1 | X)
\end{align}
$$
</p>

<p>In linear regression, we model the expected value (the mean $\mu$) of the continuous target variable $Y$ as a linear combination of the predictor vector $X$ and estimate the weight parameters $\beta_1, \beta_2, \cdots, \beta_p$ using our training data.</p>

<p class="math">
$$
\begin{align}
  \mathbb{E}(Y|X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \label{eqn:true-regression}
\end{align}
$$
</p>

<p>In this case where our target variable $Y$ is categorical and has two possible values coded as 0 and 1, the expected value or mean of $Y$ is the probability $p(X)$ of observing the positive class<sup id="fnref:expectation" role="doc-noteref"><a href="#fn:expectation" class="footnote">1</a></sup>. It seems sensible then to model the expected value of our categorical $Y$ variable using equation \eqref{eqn:true-regression}, as in linear regression.</p>

<p class="math">
$$
\begin{align}
  \mathbb{E}(Y|X) = p(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \label{eqn:probability-linear}
\end{align}
$$
</p>

<p>The problem with modelling the probability $p(X)$ as a linear combination of the predictor variables is that probability $p(X)$ has a range $[0, 1]$, but the right-hand side of the equation outputs values in the range $(-\infty, +\infty)$. In other words, we will get meaningless estimates of the probability if we use that equation.<br />
The solution is to use a function of probability $p(X)$ that provides a suitable relationship between the linear combination of the predictor variables $X$ and $p(X)$, the mean of the response variable. This function is called a link function, and it maps the probability range $[0, 1]$ to $(-\infty, +\infty)$.<br />
The most commonly used link function for binary logistic regression is the logit function (or log-odds<sup id="fnref:odds" role="doc-noteref"><a href="#fn:odds" class="footnote">2</a></sup>), given as:</p>
<p class="math">
$$
\begin{align}
  \text{logit}\bigg(p(X)\bigg) = \text{log}\bigg(\frac{p(X)}{1-p(X)}\bigg) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \label {eqn:logit}
\end{align}
$$
</p>

<p>How do we then go from the logit function to getting the estimate of the probability p(X) of observing the positive class? Because logit is a function of probability, we can take its inverse to map arbitrary values in the range $(-\infty, +\infty)$ back to the probability range $[0, 1]$. <br />
Recall that the inverse function of the natural logarithm function is the exponential function, so if we take the inverse of equation \eqref{eqn:logit}, we get:</p>

<p class="math">
$$
\begin{align}
  \frac{p(X)}{1-p(X)} = e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)} \label{eqn:odds}
\end{align}
$$
</p>

<p>If we solve for $p(X)$ in equation \eqref{eqn:odds}, we get<sup id="fnref:logistic" role="doc-noteref"><a href="#fn:logistic" class="footnote">3</a></sup>:</p>

<p class="math">
$$
\begin{align}
  p(X) &amp; = &amp; \frac{e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}}{e^{(\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)} + 1} \nonumber \\
      &amp; = &amp; \frac{1}{1 + e^{(-\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p)}} \label{eqn:logistic}
\end{align}
$$
</p>

<p>Equation \eqref{eqn:logistic} is the logistic (or sigmoid) function, and it maps values in the logit range $(-\infty, +\infty)$ back into the range $[0, 1]$ of probabilities.</p>

<p><img src="/img/log-odds.svg" alt="log-odds&amp;sigmoid" /></p>

<h3 id="deriving-cost-entropy-using-mle">Deriving Cost Entropy using MLE</h3>
<p>Given a set of $n$ training examples $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(n)}, y^{(n)})\}$, binary cross-entropy is given by:</p>

<p class="math">
$$
\begin{align}
  \text{Cross-Entropy} = -\bigg[\frac{1}{n} \sum_{i=1}^n\bigg(y^{(i)}\text{log} p^{(i)} + (1- y^{(i)})\text{log}(1 - p^{(i)})\bigg)\bigg] \label{eqn:cross-entropy}
\end{align} 
$$
</p>

<p>where $x^{(i)}$ is the feature vector, $y^{(i)}$ is the true label (0 or 1) for the $i^{th}$ training example, and $p^{(i)}$ is the predicted probability that the $i^{th}$ training example belongs to the positive class, that is, $Pr(Y = 1 | X = x^{(i)})$.</p>

<p>In this section, we will derive cross-entropy using MLE. If you are not already familiar with MLE and likelihood function, I will advise that you read the section that explains both concepts in <a href="/deriving-ml-cost-functions-part1">Part I</a> of this article.</p>

<p>The derivation of cross-entropy follows from using MLE to estimate the parameters $\beta_0, \beta_1, \cdots, \beta_p$ of our logistic model on our training data.<br />
We start by describing the random process that generated $y^{(i)}$. <br />
$y^{(i)}$ is a realisation of the Bernoulli random variable<sup id="fnref:bernoulli" role="doc-noteref"><a href="#fn:bernoulli" class="footnote">4</a></sup> $Y$. The Bernoulli distribution is parameterised by $p$, and its probability mass function (pmf) is given by:</p>

<p class="math">
$$
\begin{align}
  Pr(Y = y^{(i)}) = \begin{cases}
   p &amp; \text{if } y^{(i)} = 1, \\
   1-p &amp; \text {if } y^{(i)} = 0.
 \end{cases}
\end{align} 
$$
</p>

<p>which can be written in the more compact form:</p>

<p class="math">
$$
\begin{align}
  Pr(Y = y^{(i)}) = p^{y^{(i)}}(1-p)^{1 - y^{(i)}} \ \text{for} \ y^{(i)} \in \{0,1\} \label{eqn:compact}
\end{align} 
$$
</p>

<p>We then define our Likelihood function. The estimates of $\beta_0, \beta_1, \cdots, \beta_p$ we choose will be the ones that maximise the likelihood function. The likelihood function is a function of our parameter $p$ given our training data:</p>

<p class="math">
$$
\begin{align}
  \mathcal{L}(p | (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(n)}, y^{(n)})) &amp; = \prod_{i=1}^n f(y^{(i)}|p) \nonumber \\
                                                     &amp; = \prod_{i=1}^n p^{y^{(i)}}(1-p)^{1 - y^{(i)}} \label{eq:likelihood}
\end{align}
$$
</p>

<p>It is easier to work with the log of the likelihood function<sup id="fnref:part1" role="doc-noteref"><a href="#fn:part1" class="footnote">5</a></sup>, called the log-likelihood, so if we take the natural logarithm of equation \eqref{eq:likelihood}, we get:</p>

<p class="math">
$$
\begin{align}
  \log \bigg(\mathcal{L}(p | y^{(1)}, y^{(2)}, \cdots, y^{(n)})\bigg) &amp; = \log\bigg( \prod_{i=1}^n p^{y^{(i)}}(1-p)^{1 - y^{(i)}} \bigg) \nonumber \\
    &amp; = \sum_{i=1}^n \log \bigg(p^{y^{(i)}}(1-p)^{1 - y^{(i)}}\bigg) \nonumber \\
    &amp; = \sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \label{eq:log-likelihood}
\end{align}
$$
</p>

<p>Recall that for our training data, $p^{(i)}$ in equation \eqref{eq:log-likelihood} is the predicted probability of the $i^{th}$ training example gotten from the logisitic function, so it is a function of the parameters $\beta_0, \beta_1, \cdots, \beta_p$. The maximum likelihood estimate $\hat \beta$ is therefore the value of the parameters that maximises the log-likelihood function.</p>

<p class="math">
$$
\begin{align}
  \hat \beta = \underset{\beta}{\operatorname{arg\,max}} \bigg[ \sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \bigg]
\end{align}
$$
</p>

<p>We also know that maximising a function is the same as minimising its negative.</p>

<p class="math">
$$
\begin{align}
  \hat \beta = \underset{\beta}{\operatorname{arg\,min}} \bigg[ -\sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \bigg]
\end{align}
$$
</p>

<p>Taking the average across our $n$ training examples, we get:</p>

<p class="math">
$$
\begin{align}
  \hat \beta = \underset{\beta}{\operatorname{arg\,min}} \bigg[\color{red}-\frac{1}{n}\sum_{i=1}^n \bigg(y^{(i)}\text{log}p^{(i)} + (1-y^{(i)})\text{log}(1-p^{(i)})\bigg) \color{black}\bigg]
\end{align}
$$
</p>

<p>which is the cross-entropy as defined in equation \eqref{eqn:cross-entropy}.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:expectation" role="doc-endnote">
      <p>$Y$ is a Bernoulli random variable. The Bernoulli distribution is a discrete probability distribution that describes processes that have only two possible outcomes: 1, with a probability of $p$ and 0, with a probability of $1 - p$. The expectation (or mean) of the Bernoulli distribution is $p$. [<a href="https://brilliant.org/wiki/bernoulli-distribution/" target="_blank">Proof]</a> <a href="#fnref:expectation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:odds" role="doc-endnote">
      <p>The odds is defined as the ratio of the probability $p$ of observing an event to the probability $1-p$ of not observing that event. $\text{odds} = \frac{p}{1-p}$. If, for example, the odds of an event happening is $o$:1, it means that the event happened $o$ times out of a total of $o+1$ occurrences. The log-odds is simply the natural logarithm of the odds. <a href="#fnref:odds" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:logistic" role="doc-endnote">
      <p>A simple derivation can be found <a href="https://qr.ae/TWTpnk" target="_blank">here</a> <a href="#fnref:logistic" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bernoulli" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> is the discrete probability distribution of a random variable that takes on two possible values: 1 with probability $p$ and 0 with probability $1-p$. An experiment modelled by the Bernoulli distribution is called a Bernoull trial. Examples of Bernoulli trials include: tossing a coin (head/tail), playing a game (winning/not winning). <a href="#fnref:bernoulli" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:part1" role="doc-endnote">
      <p>The reasons why this is the case is explained clearly in <a href="/deriving-ml-cost-functions-part1">Part I</a>. Check the ‘What is Maximum Likelihood Estimation?’ section. <a href="#fnref:part1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


	</div>

	<div class="footer">
		<!-- Share Buttons -->
		<ul id="share" class="clearfix">
    <li>
        <a href="https://www.linkedin.com/shareArticle?url=http://allenkunle.me/deriving-ml-cost-functions-part2&mini=true&title=Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II&summary=In Part I of this article, we introduced Maximum Likelihood Estimation (MLE), Likelihood function, and derived Mean Squared Error (MSE) using Maximum likelihood estimation. In this article, we will use Maximum likelihood estimation to derive Cross-Entropy cost function, which is commonly used for binary classification problems. Background Binary logistic regression is used to model the relationship between a categorical target variable $Y$ and a predictor vector $X = (X_1, X_2, \cdots, X_p)$. The target variable will have two possible values, such as whether a student passes an exam or not, or whether a visitor to a website subscribes to the..."
          target="_blank" title="Share on LinkedIn"><i class="fa fa-linkedin-square"></i> Share on LinkedIn</a>
      </li>
  <li>
    <a href="https://www.twitter.com/intent/tweet?text='Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part II'&url=http://allenkunle.me/deriving-ml-cost-functions-part2&via=allenakinkunle"
      rel="nofollow" target="_blank" title="Share on Twitter"><i class="fa fa-twitter-square"></i> Share on Twitter</a>
  </li>
  <li>
    <a href="https://www.facebook.com/sharer.php?u=http://allenkunle.me/deriving-ml-cost-functions-part2" rel="nofollow"
      target="_blank" title="Share on Facebook"><i class="fa fa-facebook-square"></i> Share on Facebook</a>
  </li>
</ul>


		<!-- Show next and previous links -->
		<div id="page-navigation" class="clearfix">


  <div class="previous clearfix">
      <i class="fa fa-chevron-left"></i>
    <a href="/deriving-ml-cost-functions-part1">Deriving Machine Learning Cost Functions using Maximum Likelihood Estimation (MLE) - Part I</a>
  </div>



  <div class="next clearfix">
      <a href="/bias-variance-decomposition">The Bias-Variance Decomposition Demystified</a>
      <i class="fa fa-chevron-right"></i>
  </div>


</div>


		<!-- Comments -->
		  <!-- Add Disqus comments. -->
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'allenakinkunle'; // required: replace example with your forum shortname
    var disqus_identifier = "/deriving-ml-cost-functions-part2";

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

	</div>
</article>


	  </main>

		  <!-- Pagination links -->
      

	  </div>

	</div>
</body>
</html>
